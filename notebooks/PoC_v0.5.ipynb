{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920ba3d5",
   "metadata": {},
   "source": [
    "## Modified approach\n",
    "\n",
    "- INSTRUCTs have the desired intended effect of changing the state of the world\n",
    "    - what is the speaker's desired ontic intended effect. What state of the world would the speaker like the listener to make happen \n",
    "- STATEMENTs have the desired intended effect of changing the state of the listener's beliefs\n",
    "    - what is the speaker's desired epistemic effect on the listener. What state of the listener's mind would the speaker like the listener to make happen \n",
    "- QUESTIONs have the desired intended effect of changing the state of the speaker's beliefs \n",
    "    - what is the speaker's desired epistemic effect on themselves. What state of the speaker's mind would the speaker like the listener to help make happen.\n",
    "    \n",
    "    \n",
    "We convert everything to a \"goal\" and have a planner figure out what needs to happen. We don't care about the action repertoire at all, unless there is ambiguity \n",
    "\n",
    "### Challenges \n",
    "\n",
    "- Issues with types and type hierarchy (is \"container\" a semantic type or is it a consultant property)\n",
    "- Issues with action parameters (arbitrarily defined, not tied to semantic types, sometimes includes properties) \n",
    "- Novelty in the CPC vs novelty in the SPC. \n",
    "- agent as a physobj vs agent as a location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e75a296",
   "metadata": {},
   "source": [
    "## IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ef574b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI,ChatAnthropic\n",
    "from langchain.chains import LLMChain\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2909a3",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c56ad7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of dictionaries, and a key, return the entry in the list that matches\n",
    "def find_dict_in_list(lst, key, target):\n",
    "    for item in lst:\n",
    "        if not key in item:\n",
    "            #print(\"Key not in Dict\")\n",
    "            return None\n",
    "        if item[key] == target:\n",
    "            return item\n",
    "    #print(\"Nothing found\")\n",
    "    return None\n",
    "\n",
    "def find_all_dicts_in_list(lst, key, target):\n",
    "    output = []\n",
    "    for item in lst:\n",
    "        if not key in item:\n",
    "            return output\n",
    "        if item[key] == target:\n",
    "            output.append(item)\n",
    "    #print(\"Nothing found\")\n",
    "    return output\n",
    "    \n",
    "\n",
    "def clean_candidates(candidates):\n",
    "    \"\"\"\n",
    "    Cleans the list of candidates to extract a list of names and a list of scores of the candidates\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    scores = []\n",
    "    for candidate in candidates:\n",
    "        name = candidate.split(\":\")[0]\n",
    "        score = float(candidate.split(\":\")[1])\n",
    "        names.append(name)\n",
    "        scores.append(score)\n",
    "        \n",
    "    return names, scores\n",
    "\n",
    "\n",
    "def prune_candidates(names, scores, threshold=0.75):\n",
    "    return [(x,y) for x,y in zip(names,scores) if y > threshold ]\n",
    "\n",
    "def select_best_candidate(names, scores, threshold=0.75):\n",
    "    \"\"\"\n",
    "    Selects best name and score above a threshold. \n",
    "    \"\"\"\n",
    "    pruned_names = []\n",
    "    pruned_scores = []\n",
    "    for n,s in zip(names,scores):\n",
    "        if s>threshold:\n",
    "            pruned_names.append(n)\n",
    "            pruned_scores.append(s)\n",
    "    \n",
    "    if pruned_names:\n",
    "        return pruned_names[pruned_scores.index(max(pruned_scores))]\n",
    "    return \"NONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a73b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More helper functions GMR\n",
    "\n",
    "def central_referent(gmr):\n",
    "    return find_dict_in_list(gmr['referents'], \"role\", \"central\")\n",
    "\n",
    "def supp_referents(gmr):\n",
    "    return find_all_dicts_in_list(gmr['referents'], \"role\", \"supplemental\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e2505",
   "metadata": {},
   "source": [
    "## Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3082051",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0)\n",
    "#llm = OpenAI(temperature=0.0)\n",
    "#llm = Anthropic(model=\"claude-instant-1.1-100k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03232a",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcf8a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (1) Speech Act Classification \n",
    "\n",
    "template_speech_act= \"\"\"\n",
    "Decide whether the utterance below from a speaker to a listener is one of \"want\", \"wantBel\", \"itk\"\n",
    "A \"want\" is an imperative statement or a request by the speaker to have the listener do an action or stop doing an action.\n",
    "An \"itk\" is a 'wh' or 'yes/no' query (what, why, when, where, who) or request from a speaker for more information from the listener about the listeners knowledge, beliefs or perceptions\n",
    "A \"wantBel\" (note the uppercase B) is a statement of fact or opinion that the speaker conveys to a listener and  expects to listener to come to believe. \n",
    "\n",
    "\n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "act:\n",
    "\"\"\"\n",
    "\n",
    "prompt_speech_act = PromptTemplate(\n",
    "    input_variables=[\"utterance\"],\n",
    "    template=template_speech_act\n",
    ")\n",
    "\n",
    "chain_speech_act = LLMChain(llm=llm, prompt=prompt_speech_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d698589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (2) Central Referents \n",
    "\n",
    "template_centralref = \"\"\"\n",
    "What is the central entity or thing (which could be a single thing or a reference to a set) that is being referred to in the below sentence?\n",
    "\n",
    "Remember, the central referent is the one single thing or object, not an action or descriptor. It is meant to capture the central real world or hypothesized item being referenced in the utterance. \n",
    "\n",
    "\n",
    "sentence: \\n{utterance}\\n \n",
    "referent:\n",
    "\"\"\"\n",
    "\n",
    "prompt_centralref = PromptTemplate(\n",
    "    input_variables=[\"utterance\"],\n",
    "    template=template_centralref\n",
    ")\n",
    "\n",
    "chain_centralref = LLMChain(llm=llm, prompt=prompt_centralref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b62dca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (3) Supporting Referents\n",
    "\n",
    "template_suppref = \"\"\"\n",
    "What are some objects (which could be a single thing or a collection of things) that is being referred to in the below sentence not including the central referent? Return as a python list.\n",
    "If none, then return empty list []. Even if only one item, return as a list.  \n",
    "Remember, the supporting referents are things or objects, not actions or descriptors. They are meant to capture the real world items being referenced in the utterance. \n",
    "\n",
    "Do NOT include objects or collections that have already been covered in the central referent. \n",
    "\n",
    "sentence: \\n{utterance}\\n \n",
    "central referent: \\n{centralref}\\n\n",
    "supporting referents (noun(s) from utterance):\n",
    "\"\"\"\n",
    "\n",
    "prompt_suppref = PromptTemplate(\n",
    "    input_variables=[\"utterance\", \"centralref\"],\n",
    "    template=template_suppref\n",
    ")\n",
    "\n",
    "chain_suppref = LLMChain(llm=llm, prompt=prompt_suppref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc9d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (4) Getting the type of thing that the referents are \n",
    "\n",
    "template_typeof = \"\"\"\n",
    "Determine whether or not the referent item mentioned below in the context of the provided utterance is one of the types also provided below. To check if the referent is of a type, follow the below procedure\n",
    "1. Iterate through each item mentioned in the list of types. \n",
    "2. For each item X in the list of types expand on the meaning of each item in context of the utterance, and then ask if the central referent is of type X given that meaning. \n",
    "3. If the central referent is of type X in the list, return X.\n",
    "\n",
    "\\n\\n EXAMPLE \\n\n",
    "utterance: The lemon is on the table\n",
    "referent: lemon\n",
    "types: ['area', 'physobj', 'location', 'pose']\n",
    "typeOf: Looking through the items in the list of types above, physobj is a physical object. lemon is a type of physical object and its use in the utterance is that of a physobj. So, it is of type physobj\n",
    "\n",
    "Remember, return specifically ONE of the items in the list, or if none apply then return NONE. \n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "referent: \\n{ref}\\n\n",
    "types: \\n{types}\\n\n",
    "typeOf:\n",
    "\"\"\"\n",
    "\n",
    "prompt_typeof = PromptTemplate(\n",
    "    input_variables=[\"ref\", \"types\", \"utterance\"],\n",
    "    template=template_typeof\n",
    ")\n",
    "\n",
    "chain_typeof = LLMChain(llm=llm, prompt=prompt_typeof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5daf694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
