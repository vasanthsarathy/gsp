{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62792189",
   "metadata": {},
   "source": [
    "## Issues\n",
    "\n",
    "- I think the system is not consistently applying the variable names. \n",
    "- Need to set up a dev dataset \n",
    "- type matching\n",
    "- need to revisit the speech simplifier -- seems to be oversimplifying.\n",
    "    - \"pick up any other ball\" --> this specifically excludes currently activated/in-focus ball from consideration\n",
    "- Need to figure out the overall robot algorithm here. \n",
    "\n",
    "\n",
    "## Features\n",
    "1. Parsing \n",
    "    - Concept learning \n",
    "2. Rule induction\n",
    "    - Commonsense \n",
    "3. Exception Induction\n",
    "4. Interactive Dialog\n",
    "    - Rule understanding (are the exceptions correct? is the rule correct?)\n",
    "    - Rule Relaxation/Conflict Resolution (selecting which rule to relax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2daa8c",
   "metadata": {},
   "source": [
    "# Algorithm \n",
    "\n",
    "## System level\n",
    "\n",
    "### func learn(utterance U, robot action domain D, robot sensory domain S, ):\n",
    "- robot asked to perform a task (action or goal instruction)\n",
    "- robot interrupted \n",
    "- robot provided some fact about why it was interrupted. \n",
    "    if no fact, then robot asks \"why\"\n",
    "- robot learns fact \n",
    "\n",
    "### func induce rule()\n",
    "- robot induces a rule and generates likely exceptions\n",
    "- robot refines rule with human interaction\n",
    "\n",
    "\n",
    "### func refine()\n",
    "- robot has a rule and exceptions and wants to check if it is correct\n",
    "- human might be able to tell them if they \n",
    "    - missing some predicate\n",
    "    - have an extra or incorrect predicate \n",
    "    - have any exceptions\n",
    "        - robot can check if it already has exceptions. \n",
    "- repeat until human is happy. \n",
    "\n",
    "Functionality needed for refinement is -- ensuring that each of the RHS stuff in the rule is a tethered concept -- i.e., there is a proof for it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1c8ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\n",
    "    {'action': 'pickup',\n",
    "    'description': 'Picks up items',\n",
    "    'parameters': ['object'], \n",
    "    },\n",
    "    {'action': 'putdown',\n",
    "    'description': 'Puts down items',\n",
    "    'parameters': ['object'], \n",
    "    },\n",
    "    {'action': 'stack',\n",
    "     'description': 'Stacks one object on top of another object',\n",
    "     'parameters': ['object', 'object']\n",
    "    }\n",
    "]\n",
    "\n",
    "consultants = [\n",
    "    {'consultant': 'vision',\n",
    "    'concepts': [\n",
    "        {'name': 'clear',\n",
    "         'description': 'Checks if an object has nothing on top of it',\n",
    "         'parameters': ['object']\n",
    "        },\n",
    "        {'name': 'holding',\n",
    "         'description': 'Checks if the agent is holding the object',\n",
    "         'parameters': ['object']\n",
    "        },\n",
    "        {'name': 'on',\n",
    "         'description': 'Checks if an object is top of another object',\n",
    "        'parameters': ['object', 'object']},\n",
    "        {'name': 'box',\n",
    "         'description': 'Checks if an object is a box',\n",
    "         'parameters': ['object']\n",
    "        }\n",
    "    ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b6c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of dictionaries, and a key, return the entry in the list that matches\n",
    "def find_dict_in_list(lst, key, target):\n",
    "    for item in lst:\n",
    "        if not key in item:\n",
    "            #print(\"Key not in Dict\")\n",
    "            return None\n",
    "        if item[key] == target:\n",
    "            return item\n",
    "    #print(\"Nothing found\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf4fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c54827b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0)\n",
    "llm = OpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40c6b4c",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d67e6a1",
   "metadata": {},
   "source": [
    "### Speech Acts\n",
    "\n",
    "A `speech act` is the action being performed by the utterance. We have several `types` of speech acts including `INSTRUCT`, `STATEMENT` etc. \n",
    "\n",
    "Note, we are talking about the nature of the utterance at the surface level. This does not distinguish between direct and indirect speech acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74c8db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (0) Speech simplifier\n",
    "\n",
    "template_simplifier= \"\"\"\n",
    "Simplify this sentence (don't get rid of determiners and articles):\n",
    "\\n\n",
    "sentence:\\n{utterance}\\n\n",
    "simplified:\n",
    "\"\"\"\n",
    "prompt_simplifier = PromptTemplate(\n",
    "    input_variables=[\"utterance\"],\n",
    "    template=template_simplifier\n",
    ")\n",
    "\n",
    "chain_simplifier = LLMChain(llm=llm, prompt=prompt_simplifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e20c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (1) Speech Act Classification \n",
    "\n",
    "template_speech_act_classifier= \"\"\"\n",
    "Decide whether the utterance below from a speaker to a listener is one of INSTRUCT, STATEMENT, GREETING, QUESTIONWH ('wh', questions), QUESTIONYN ('yes/no' questions), ACK (e.g. \"yes\" or \"ok\"), or UNKNOWN \n",
    "An INSTRUCT is an imperative statement or a request by the speaker to have the listener do an action or stop doing an action.\n",
    "A QUESTIONWH is a 'wh' query (what, why, when, where, who) or request from a speaker for more information from the listener about the listeners knowledge, beliefs or perceptions\n",
    "A QUESTIONYN is a 'yes/no' query or request from a speaker for more information from the listener about the listeners knowledge, beliefs or perceptions, but the speaker expects a yes or no for an answer\n",
    "A STATEMENT is a statement of fact or opinion that the speaker conveys to a listener. \n",
    "A GREETING is an expression of social connection establishing the start of the conversation. E.g., \"Hello\"\n",
    "A ACK is an acknowledgement (either \"yes\" or \"no\").\n",
    "A UNKNOWN is an utterance not one of the above. \n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "act:\n",
    "\"\"\"\n",
    "\n",
    "prompt_speech_act_classifier = PromptTemplate(\n",
    "    input_variables=[\"utterance\"],\n",
    "    template=template_speech_act_classifier\n",
    ")\n",
    "\n",
    "chain_speech_act_classifier = LLMChain(llm=llm, prompt=prompt_speech_act_classifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ad294",
   "metadata": {},
   "source": [
    "### Principle Intended Effect\n",
    "\n",
    "The `principle intended effect` or `pie`, is what the speaker intends for the listener to do with the communicated utterance. This is a pragmatics issue and as such needs a mapping from the `speech act type` to the `pie`. Examples of `pie` include things like `want` and `wantBel` (where the speaker wants the listener to perform an action or speaker wants the listener to come believe something). Note, this is where reasoning about the indirect speech act comes into play. A `speech act type` of `question` (e.g., can you pass the salt) might result in the `pie` being `want` as opposed to `tellAnswer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8be3d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node code yet for PIE -- we can just have pragmatics handle that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cefa47",
   "metadata": {},
   "source": [
    "### Actions and Concepts\n",
    "\n",
    "#### Actions\n",
    "\n",
    "`Actions` are symbolically specified functions that can be executed by the agent. Each `action` has an associated `action signature` which in turn has a `name`, `parameters` and `description`. For example: \n",
    "\n",
    "```\n",
    "Action Definition:\n",
    "    {'name': 'stack',\n",
    "     'description': 'Stacks one object on top of another object',\n",
    "     'parameters': ['object', 'object']\n",
    "    }\n",
    "```\n",
    "\n",
    "The `name` is simply a string without spaces or special characters. The `description` is mpre general string containing an NL description of what the action is about. The `parameters` is an ordered list of strings. Each item in the list is a `parameter type symbol` that specifies the `type` that a `variable symbol` and also an `object constant symbol` can take. Note, this is just how actions are defined. Grounded actions have `parameters` that are constant symbols. See below. \n",
    "\n",
    "#### Concepts\n",
    "\n",
    "`Concepts` are symbolically specified predicates that can be explicitly tracked by the agent. Each `concept` also has an associated `concept signature` which in turn has a `name`, `parameters` and `description`. For example: \n",
    "\n",
    "```\n",
    "Concept:\n",
    "    {'name': 'on',\n",
    "     'description': 'Checks if an object is top of another object',\n",
    "     'parameters': ['object', 'object']\n",
    "    }\n",
    "```\n",
    "\n",
    "#### Grounded vs Lifted\n",
    "\n",
    "A `grounded action` is an action in which all the parameters are replaced with specific `object constant symbols`. Similarly a `grounded concept` is a concept in which all parameters are replaced with specific `object constant symbols`. Note, for a grounded action or concept to be type-matched, the `object constant symbols` must have a `type` that matches the parameter `type` specified in the action/concept definition. \n",
    "\n",
    "A `lifted action/concept` is an action/concept in which one or more parameters are `variable symbols`. Here too, the parameters MUST be type-matched, i.e., any variable or object `constant symbols` must have a `type` that matches the parameter `type` specified in the action/concept definition. \n",
    "\n",
    "#### Tethered Action/Concept\n",
    "\n",
    "A `tethered action` is an action that the robot has the ability to execute. A `tethered concept` is a concept that the robot can perceive its truth value from the current state. \n",
    "\n",
    "#### Novel Action/Concept \n",
    "\n",
    "A `novel action or concept` is an action or concept that the robot does not already knowledge of. It is not present in the agent's action database or set of conceptual beliefs or consultant capabilities.\n",
    "\n",
    "**Note:**: One could think about derived predicates, i.e., concepts that are _only_ defined as the head of horn clauses in Belief. One could argue that these are ungrounded in a way, but they are tethered because they can be infered. But, I am arguing here is that these are defined in terms  of other concepts and as long as they are tethered to a grounded concept, we should be okay. \n",
    "\n",
    "\n",
    "Let's walk through an example:\n",
    "```\n",
    "    Utterance: Evan owns the purple box. \n",
    "\n",
    "    Concepts: \n",
    "    own(obj35, evan), purple(obj35), box(obj35), agent(evan)\n",
    "    \n",
    "```\n",
    "Let's say the robot has an implemented \"purple\" detector and a \"box\" detector. It also knows that evan is an agent. \n",
    "\n",
    "Let's say the robot has no prior knowledge of \"own(X)\", which means it is a new concept. \"own(X)\" is an untethered concept that is also novel. When asserted by the human, the robot will come to believe that \"own(obj35, evan)\", but it cannot infer whether another object is owned by evan or other objects are owned by others.\n",
    "\n",
    "We say here that \"own(X)\" is `untethered` but that doesn't stop the robot from reasoning about it. Crucially, if it has another rule: \n",
    "\n",
    "```\n",
    "constraintViolation(O) :- own(O,A), holding(O), agent(A), object(O), A!=self. \n",
    "```\n",
    "With this rule and the above fact, the agent can reason that they have violated a constraint, without really understanding what \"ownership\" actually means. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2092e5e",
   "metadata": {},
   "source": [
    "### Propositional Content (PC)\n",
    "\n",
    "#### Core Propositional Content (CPC)\n",
    "The `core propositional content` or `cpc` has to do with the \"thing\" that the speaker is talking about. Searle calls this the propositional content condition. For example, \"The purple box is open\" has a `cpc` of \"isOpen(X)\". \"Open the purple box\" has a `cpc` of \"open(X)\". Note here that \"open\" is an action whereas \"isOpen\" is a concept. We will talk about actions and concepts later. \n",
    "\n",
    "#### Supporting Propositional Content (SPC) \n",
    "The `supporting propositional content` or `spc` has to do with constraining information about the `cpc`. So, \"The purple box is open\" has `spc` of {purple(X), box(X), DEFINITE(X)}. Similarly, \"Open the purple box\" has an `spc` of {purple(X), box(X), DEFINITE(X)}. Note the `spc` is the same in both cases. Note, also that the `spc` is a collection of concepts. But it could be more complicated like \"Open the purple box you had opened and closed yesterday\". Again, the `cpc` is still \"open(X)\", but now the `spc` is more complex {purple(X), box(X), DEFINITE(X), ...history(X,\\[open(X), close(X)\\])...}\n",
    "\n",
    "Let's start with the `cpc`: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7449aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPC determination\n",
    "\n",
    "## (2a) CPC associated with concepts because we have a STATEMENT Speech act. \n",
    "\n",
    "template_cpc_concept_selector=\"\"\"\n",
    "Generate a single string (without spaces or other special characters) that represents the core propositional content of the utterance. \n",
    "Use the following procedure:\n",
    "1. Select the core portion of the utterance that represents the property of an object or concept associated with the object that the speaker is telling the listener that the listener does not already know.\n",
    "2. Then, compare the name and description of each of the properties below to the utterance. Narrow the list of properties to include only those with a semantically similar name or description to the core portion of the utterance. \n",
    "3. If the narrowed list contains one property, then return its name. If it contains no properties, then generate a new symbol -- a short (4-5 character) string to represent this portion/property in the utterance. If it contains more than one property, then return AMBIGUOUS.\n",
    "\n",
    "\\n\\n LIST OF AVAILABLE PROPERTIES/CONCEPTS \\n:\n",
    "{concepts}\n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "core proposition name:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_cpc_concept_selector = PromptTemplate(\n",
    "    input_variables=[\"utterance\", \"concepts\"],\n",
    "    template=template_cpc_concept_selector\n",
    ")\n",
    "\n",
    "chain_cpc_concept_selector = LLMChain(llm=llm, prompt=prompt_cpc_concept_selector)\n",
    "\n",
    "##---------\n",
    "# (2b) CPC associated with Actions because we have an INSTRUCT action\n",
    "\n",
    "template_cpc_action_selector= \"\"\"\n",
    "Select an action from the list of available actions that is most relevant to the given utterance. \n",
    "To decide the applicable action, use the following procedure to systematically filter the most relevant action:\n",
    "1. Check the dialog_type. Narrow the list of actions to only include those actions that are ontic actions or world-modifying actions. \n",
    "\n",
    "2. Then, compare the name and description of the action to the utterance. Narrow the list of actions to include only those with a semantically similar name or description to the utterance. \n",
    "\n",
    "3. If the narrowed list contains one action, then return its name. If it contains no actions, then return NONE. If it contains more than one action, then return AMBIGUOUS.\n",
    "\n",
    "\n",
    "\\n\\n LIST OF AVAILABLE ACTIONS \\n:\n",
    "{actions}\n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "action:\n",
    "\"\"\"\n",
    "\n",
    "prompt_cpc_action_selector = PromptTemplate(\n",
    "    input_variables=[\"utterance\", \"actions\"],\n",
    "    template=template_cpc_action_selector\n",
    ")\n",
    "\n",
    "chain_cpc_action_selector = LLMChain(llm=llm, prompt=prompt_cpc_action_selector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "094f3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPC determiner \n",
    "\n",
    "## (4) Identify a referrent object and its properties. \n",
    "### Given an utterance, an action or a property (the core proposition), and its parameter types, return a set of properties \n",
    "\n",
    "template_referent_properties= \"\"\"\n",
    "Identify descriptors in the utterance that refer to an entity which is an argument or parameter in the core proposition of the utterance.\n",
    "Use the following procedure:\n",
    "1. For each of the parameter types, identify what descriptive terms refer to the parameter type from the utterance. \n",
    "That is think of an entity is being referred to by the parameter types, and see what descriptors refer to this entity.\n",
    "2. Build a predicates for each descriptive terms. The predicates have a functor name that is the descriptor and a variable name.\n",
    "Hypothesize variable names (e.g., VAR0, VAR1, etc.). The predicates should be of the form `descriptive term(variable names)`\n",
    "3. Compose the predicates into a list \n",
    "4. Return the list as properties. \n",
    "\n",
    "Remember, these are descriptive properties (adjectives in a sense) and do NOT include cues for adverbs, articles, determiners, pronouns etc.\n",
    "Also remember that functors cannot contain spaces, so replace spaces with underscores.\n",
    "Also, remember the output list of properties should not include the core proposition itself. \n",
    "\\n\\nEXAMPLE\\n\n",
    "utterance: Pick up the blue ball\n",
    "core proposition: pickup\n",
    "parameter types: [\"object\"]\n",
    "properties: [\"blue(VAR0)\", \"ball(VAR0)\"]\n",
    "\\n\\n\n",
    "\\n\\nEXAMPLE\\n\n",
    "utterance: Stack the cup on any other cup\n",
    "core proposition: stack\n",
    "parameter types: [\"object\",\"object\"]\n",
    "properties: [\"cup(VAR0)\", \"cup(VAR1)\"]\n",
    "\\n\\n\n",
    "utterance: \\n{utterance}\\n\n",
    "core proposition: \\n{cpc}\\n\n",
    "parameter types: \\n{parameters}\\n\n",
    "properties:\n",
    "\"\"\"\n",
    "prompt_referent_properties = PromptTemplate(\n",
    "    input_variables=[\"utterance\", \"cpc\", \"parameters\"],\n",
    "    template=template_referent_properties\n",
    ")\n",
    "\n",
    "chain_referent_properties = LLMChain(llm=llm, prompt=prompt_referent_properties)\n",
    "\n",
    "\n",
    "\n",
    "## (5) Ref Res: For each variable determine it's cognitive status \n",
    "### For each variable we are figuring out the one or more givenness hierarchy statuses\n",
    "\n",
    "template_cognitive_status= \"\"\"\n",
    "For each variable in the properties, decide which ONE (and only one) of the following five cognitive statuses the variables in the below properties could fall into:\n",
    "statuses: [INFOCUS, ACTIVATED\", FAMILIAR, DEFINITE, INDEFINITE]\n",
    "\n",
    "As shown in the table below, the Givenness Hierarchy is comprised of six hierarchically nested tiers of cognitive status, \n",
    "where information with one cognitive status can be inferred to also have all\n",
    "lower statuses. Each level of the GH is “cued” by a set\n",
    "of linguistic forms, as seen in the table. For example, the second\n",
    "row of the table shows that the definite use of “this” can be\n",
    "used to infer that the speaker assumes the referent to be at\n",
    "least activated to their interlocutor.\n",
    "\\n\\n\n",
    "Cognitive Status | Mnemonic Status | Form |\n",
    "-----------------|-----------------|------|\n",
    "INFOCUS | in the focus of attention | it |\n",
    "ACTIVATED | in short term memory | this,that,this N |\n",
    "FAMILIAR | in long term memory| that N |\n",
    "DEFINITE | in long term memory  or new | the N |\n",
    "INDEFINITE | new or hypothetical | a N |\n",
    "\\n\\n\n",
    "\n",
    "When deciding the one cognitive status for each variable, use the table above and compare the form (pronoun, determiner, article) of the utterance to its status.\n",
    "\\n\\nExample:\\n\n",
    "utterance: Pick up the blue ball\n",
    "properties: [\"blue(VAR0)\", \"ball(VAR0)\"]\n",
    "cognitive status: [\"DEFINITE(VAR0)\"]\n",
    "\\n\\n\n",
    "utterance: \\n{utterance}\\n\n",
    "properties: \\n{properties}\\n\n",
    "cognitive status:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt_cognitive_status = PromptTemplate(\n",
    "    input_variables=[\"utterance\", \"properties\"],\n",
    "    template=template_cognitive_status\n",
    ")\n",
    "\n",
    "chain_cognitive_status = LLMChain(llm=llm, prompt=prompt_cognitive_status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b8a36a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "def parse_utterance(utterance, actions, properties):\n",
    "    #print(f\"Utterance: {utterance}\")\n",
    "    #print(f\"Actions: {actions}\")\n",
    "    # utterance = chain_simplifier.run(utterance=utterance)\n",
    "    speech_act = chain_speech_act_classifier.run(utterance=utterance)\n",
    "    if speech_act == 'INSTRUCT':\n",
    "        cpc = chain_cpc_action_selector.run(utterance=utterance, actions=actions)\n",
    "    elif speech_act == 'STATEMENT':\n",
    "        cpc = chain_cpc_concept_selector.run(utterance=utterance,concepts=consultants)\n",
    "    else:\n",
    "        return {} \n",
    "        \n",
    "    parameters = []\n",
    "    properties = []\n",
    "    cognitive_status = []\n",
    "    variables = []\n",
    "    if not cpc==\"NONE\" and not cpc==\"AMBIGUOUS\":\n",
    "        # Get params\n",
    "        if speech_act == 'INSTRUCT':\n",
    "            relevant_cpc = find_dict_in_list(actions, 'action', cpc)\n",
    "        elif speech_act == 'STATEMENT':\n",
    "            print(cpc)\n",
    "            print(consultants[0]['concepts'])\n",
    "            relevant_cpc = find_dict_in_list(consultants[0]['concepts'], 'name', cpc)\n",
    "        else: \n",
    "            return {}\n",
    "        parameters = relevant_cpc['parameters']\n",
    "        \n",
    "        # Get properties\n",
    "        properties = chain_referent_properties.run(utterance=utterance, cpc=cpc, parameters=parameters)\n",
    "        properties = ast.literal_eval(properties)\n",
    "        \n",
    "        # Get list of variable names\n",
    "        def get_args(predicate):\n",
    "            return re.search('\\(([^)]+)', predicate).group(1)\n",
    "        \n",
    "        if properties:\n",
    "            variables = list(set([get_args(x) for x in properties]))\n",
    "        \n",
    "        \n",
    "        cognitive_status = chain_cognitive_status.run(utterance=utterance, properties=properties)\n",
    "        \n",
    "        #eval cog status\n",
    "        cognitive_status = ast.literal_eval(cognitive_status)\n",
    "        \n",
    "    output = {'utterance': utterance,\n",
    "              'speech_act':speech_act,\n",
    "              'cpc': cpc,\n",
    "             'parameters': parameters,\n",
    "             'properties': properties,\n",
    "              'variables': variables,\n",
    "             'cognitive_status': cognitive_status}\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def parse(speaker, listener, utterance, actions, consultants):\n",
    "    output = parse_utterance(utterance, actions, consultants)\n",
    "    variables = \",\".join(output['variables'])\n",
    "    properties = \",\".join(output['properties'])\n",
    "    cognitive_status = \",\".join(output['cognitive_status'])\n",
    "    cpc = output['cpc']\n",
    "    speech_act = output['speech_act']\n",
    "    \n",
    "    template = \"{speech_act}({speaker},{listener},{cpc}({variables}),{{{properties},{cognitive_status}}})\"\n",
    "    parsed = template.format(speech_act=speech_act,\n",
    "                            speaker=speaker,\n",
    "                            listener=listener,\n",
    "                            cpc=cpc,\n",
    "                            variables=variables,\n",
    "                            properties=properties,\n",
    "                            cognitive_status=cognitive_status)\n",
    "        \n",
    "    return parsed, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94549a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belong\n",
      "[{'name': 'clear', 'description': 'Checks if an object has nothing on top of it', 'parameters': ['object']}, {'name': 'holding', 'description': 'Checks if the agent is holding the object', 'parameters': ['object']}, {'name': 'on', 'description': 'Checks if an object is top of another object', 'parameters': ['object', 'object']}, {'name': 'box', 'description': 'Checks if an object is a box', 'parameters': ['object']}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vsarathy/.cache/pypoetry/virtualenvs/gsp-KcVP1gnp-py3.10/lib/python3.10/site-packages/gradio/routes.py\", line 414, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/vsarathy/.cache/pypoetry/virtualenvs/gsp-KcVP1gnp-py3.10/lib/python3.10/site-packages/gradio/blocks.py\", line 1320, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/vsarathy/.cache/pypoetry/virtualenvs/gsp-KcVP1gnp-py3.10/lib/python3.10/site-packages/gradio/blocks.py\", line 1048, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/vsarathy/.cache/pypoetry/virtualenvs/gsp-KcVP1gnp-py3.10/lib/python3.10/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/vsarathy/.cache/pypoetry/virtualenvs/gsp-KcVP1gnp-py3.10/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/vsarathy/.cache/pypoetry/virtualenvs/gsp-KcVP1gnp-py3.10/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/tmp/ipykernel_325431/956771686.py\", line 4, in nlu\n",
      "    parsed, output = parse(\"brad\", \"self\", utterance, actions, consultants)\n",
      "  File \"/tmp/ipykernel_325431/1051125762.py\", line 61, in parse\n",
      "    output = parse_utterance(utterance, actions, consultants)\n",
      "  File \"/tmp/ipykernel_325431/1051125762.py\", line 30, in parse_utterance\n",
      "    parameters = relevant_cpc['parameters']\n",
      "TypeError: 'NoneType' object is not subscriptable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belong\n",
      "[{'name': 'clear', 'description': 'Checks if an object has nothing on top of it', 'parameters': ['object']}, {'name': 'holding', 'description': 'Checks if the agent is holding the object', 'parameters': ['object']}, {'name': 'on', 'description': 'Checks if an object is top of another object', 'parameters': ['object', 'object']}, {'name': 'box', 'description': 'Checks if an object is a box', 'parameters': ['object']}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vsarathy/.cache/pypoetry/virtualenvs/gsp-KcVP1gnp-py3.10/lib/python3.10/site-packages/gradio/routes.py\", line 414, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/vsarathy/.cache/pypoetry/virtualenvs/gsp-KcVP1gnp-py3.10/lib/python3.10/site-packages/gradio/blocks.py\", line 1320, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/vsarathy/.cache/pypoetry/virtualenvs/gsp-KcVP1gnp-py3.10/lib/python3.10/site-packages/gradio/blocks.py\", line 1048, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/home/vsarathy/.cache/pypoetry/virtualenvs/gsp-KcVP1gnp-py3.10/lib/python3.10/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/vsarathy/.cache/pypoetry/virtualenvs/gsp-KcVP1gnp-py3.10/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/vsarathy/.cache/pypoetry/virtualenvs/gsp-KcVP1gnp-py3.10/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/tmp/ipykernel_325431/956771686.py\", line 4, in nlu\n",
      "    parsed, output = parse(\"brad\", \"self\", utterance, actions, consultants)\n",
      "  File \"/tmp/ipykernel_325431/1051125762.py\", line 61, in parse\n",
      "    output = parse_utterance(utterance, actions, consultants)\n",
      "  File \"/tmp/ipykernel_325431/1051125762.py\", line 30, in parse_utterance\n",
      "    parameters = relevant_cpc['parameters']\n",
      "TypeError: 'NoneType' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def nlu(utterance):\n",
    "    parsed, output = parse(\"brad\", \"self\", utterance, actions, consultants)\n",
    "    return parsed, output\n",
    "\n",
    "demo = gr.Interface(fn=nlu, inputs=\"text\", outputs=[\"text\", \"text\"])\n",
    "\n",
    "demo.launch(debug=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca92f4d",
   "metadata": {},
   "source": [
    "## Run a short dev test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9790dcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"utterance\": \"Pick up the ball\",\n",
      "  \"speech_act\": \"INSTRUCT\",\n",
      "  \"cpc\": \"pickup\",\n",
      "  \"parameters\": [\n",
      "    \"object\"\n",
      "  ],\n",
      "  \"properties\": [\n",
      "    \"ball(VAR0)\"\n",
      "  ],\n",
      "  \"variables\": [\n",
      "    \"VAR0\"\n",
      "  ],\n",
      "  \"cognitive_status\": [\n",
      "    \"DEFINITE(VAR0)\"\n",
      "  ]\n",
      "}\n",
      "PARSED:  INSTRUCT(brad,self,pickup(VAR0),{ball(VAR0),DEFINITE(VAR0)}) \n",
      "\n",
      "{\n",
      "  \"utterance\": \"Get the ball\",\n",
      "  \"speech_act\": \"INSTRUCT\",\n",
      "  \"cpc\": \"NONE\",\n",
      "  \"parameters\": [],\n",
      "  \"properties\": [],\n",
      "  \"variables\": [],\n",
      "  \"cognitive_status\": []\n",
      "}\n",
      "PARSED:  INSTRUCT(brad,self,NONE(),{,}) \n",
      "\n",
      "{\n",
      "  \"utterance\": \"Pick up a ball\",\n",
      "  \"speech_act\": \"INSTRUCT\",\n",
      "  \"cpc\": \"pickup\",\n",
      "  \"parameters\": [\n",
      "    \"object\"\n",
      "  ],\n",
      "  \"properties\": [\n",
      "    \"ball(VAR0)\"\n",
      "  ],\n",
      "  \"variables\": [\n",
      "    \"VAR0\"\n",
      "  ],\n",
      "  \"cognitive_status\": [\n",
      "    \"INDEFINITE(VAR0)\"\n",
      "  ]\n",
      "}\n",
      "PARSED:  INSTRUCT(brad,self,pickup(VAR0),{ball(VAR0),INDEFINITE(VAR0)}) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"utterance\": \"Pick up this blue ball\",\n",
      "  \"speech_act\": \"INSTRUCT\",\n",
      "  \"cpc\": \"pickup\",\n",
      "  \"parameters\": [\n",
      "    \"object\"\n",
      "  ],\n",
      "  \"properties\": [\n",
      "    \"this(VAR0)\",\n",
      "    \"blue(VAR0)\",\n",
      "    \"ball(VAR0)\"\n",
      "  ],\n",
      "  \"variables\": [\n",
      "    \"VAR0\"\n",
      "  ],\n",
      "  \"cognitive_status\": [\n",
      "    \"ACTIVATED(VAR0)\"\n",
      "  ]\n",
      "}\n",
      "PARSED:  INSTRUCT(brad,self,pickup(VAR0),{this(VAR0),blue(VAR0),ball(VAR0),ACTIVATED(VAR0)}) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"utterance\": \"Put the ball down on the table\",\n",
      "  \"speech_act\": \"INSTRUCT\",\n",
      "  \"cpc\": \"putdown\",\n",
      "  \"parameters\": [\n",
      "    \"object\"\n",
      "  ],\n",
      "  \"properties\": [\n",
      "    \"ball(VAR0)\",\n",
      "    \"table(VAR0)\"\n",
      "  ],\n",
      "  \"variables\": [\n",
      "    \"VAR0\"\n",
      "  ],\n",
      "  \"cognitive_status\": [\n",
      "    \"DEFINITE(VAR0)\"\n",
      "  ]\n",
      "}\n",
      "PARSED:  INSTRUCT(brad,self,putdown(VAR0),{ball(VAR0),table(VAR0),DEFINITE(VAR0)}) \n",
      "\n",
      "{\n",
      "  \"utterance\": \"Get the circuit breaker on the work area\",\n",
      "  \"speech_act\": \"INSTRUCT\",\n",
      "  \"cpc\": \"NONE\",\n",
      "  \"parameters\": [],\n",
      "  \"properties\": [],\n",
      "  \"variables\": [],\n",
      "  \"cognitive_status\": []\n",
      "}\n",
      "PARSED:  INSTRUCT(brad,self,NONE(),{,}) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"utterance\": \"First raise your arms\",\n",
      "  \"speech_act\": \"INSTRUCT\",\n",
      "  \"cpc\": \"NONE\",\n",
      "  \"parameters\": [],\n",
      "  \"properties\": [],\n",
      "  \"variables\": [],\n",
      "  \"cognitive_status\": []\n",
      "}\n",
      "PARSED:  INSTRUCT(brad,self,NONE(),{,}) \n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutterance\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPick up the ball\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mact\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINSTRUCT\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      4\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutterance\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGet the ball\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mact\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINSTRUCT\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      5\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutterance\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPick up a ball\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mact\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINSTRUCT\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m        {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutterance\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe object in front of you is a ball\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mact\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTATEMENT\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     14\u001b[0m        {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutterance\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt uses a medical caddy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mact\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTATEMENT\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m---> 17\u001b[0m     parsed, output \u001b[38;5;241m=\u001b[39m \u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mself\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutterance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsultants\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(output, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))   \n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPARSED: \u001b[39m\u001b[38;5;124m\"\u001b[39m, parsed,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 62\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(speaker, listener, utterance, actions, consultants)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(speaker, listener, utterance, actions, consultants):\n\u001b[1;32m     61\u001b[0m     output \u001b[38;5;241m=\u001b[39m parse_utterance(utterance, actions, consultants)\n\u001b[0;32m---> 62\u001b[0m     variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvariables\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     63\u001b[0m     properties \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     64\u001b[0m     cognitive_status \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcognitive_status\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'variables'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = [{\"utterance\": \"Pick up the ball\", \"act\": \"INSTRUCT\"},\n",
    "        {\"utterance\": \"Get the ball\", \"act\": \"INSTRUCT\"},\n",
    "        {\"utterance\": \"Pick up a ball\", \"act\": \"INSTRUCT\"},\n",
    "        {\"utterance\": \"Pick up this blue ball\", \"act\": \"INSTRUCT\"},\n",
    "        {\"utterance\": \"Put the ball down on the table\", \"act\":\"INSTRUCT\"},\n",
    "       {\"utterance\": \"Get the circuit breaker on the work area\", \"act\": \"INSTRUCT\"},\n",
    "       {\"utterance\": \"First raise your arms\", \"act\": \"INSTRUCT\"},\n",
    "       {\"utterance\": \"Dempster, who do you trust?\", \"act\": \"QUESTION\"},\n",
    "       {\"utterance\": \"Dempster, do you see an object?\", \"act\": \"QUESTION\"},\n",
    "       {\"utterance\": \"The area behind you is safe\", \"act\": \"STATEMENT\"},\n",
    "       {\"utterance\": \"The object in front of you is a ball\", \"act\": \"STATEMENT\"},\n",
    "       {\"utterance\": \"It uses a medical caddy\", \"act\": \"STATEMENT\"}]\n",
    "\n",
    "for item in data:\n",
    "    parsed, output = parse(\"brad\", \"self\", item['utterance'], actions, consultants)\n",
    "    print(json.dumps(output, indent=2))   \n",
    "    print(\"PARSED: \", parsed,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f61bc1e",
   "metadata": {},
   "source": [
    "# DO NOT USE BELOW THIS LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273036bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot testing\n",
    "data = [{\"utterance\": \"Pick up the ball\", \"act\": \"INSTRUCT\"},\n",
    "        {\"utterance\": \"Get the ball\", \"act\": \"INSTRUCT\"},\n",
    "        {\"utterance\": \"Pick up a ball\", \"act\": \"INSTRUCT\"},\n",
    "        {\"utterance\": \"Pick up this blue ball\", \"act\": \"INSTRUCT\"},\n",
    "        {\"utterance\": \"Put the ball down on the table\", \"act\":\"INSTRUCT\"},\n",
    "       {\"utterance\": \"Get the circuit breaker on the work area\", \"act\": \"INSTRUCT\"},\n",
    "       {\"utterance\": \"First raise your arms\", \"act\": \"INSTRUCT\"},\n",
    "       {\"utterance\": \"Dempster, who do you trust?\", \"act\": \"QUESTION\"},\n",
    "       {\"utterance\": \"Dempster, do you see an object?\", \"act\": \"QUESTION\"},\n",
    "       {\"utterance\": \"The area behind you is safe\", \"act\": \"STATEMENT\"},\n",
    "       {\"utterance\": \"The object in front of you is a ball\", \"act\": \"STATEMENT\"},\n",
    "       {\"utterance\": \"It uses a medical caddy\", \"act\": \"STATEMENT\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d0548",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Explanations\n",
    "### Given a set of instructions that produced a parse, generate an argument for the parse is correct. \n",
    "\n",
    "template_argument = \"\"\"\n",
    "Generate an argume\n",
    "Cognitive Status | Mnemonic Status | Form |\n",
    "-----------------|-----------------|------|\n",
    "INFOCUS | in the focus of attention | it |\n",
    "ACTIVATED | in short term memory | this,that,this N |\n",
    "FAMILIAR | in long term memory| that N |\n",
    "UNIQUELY IDENTIFIABLE | in long term memory  or new | the N |\n",
    "REFERENTIAL | nt for why the system came up with the particular parse of the utterance. \n",
    "To generate this argument ONLY use the below information, which contains prior prompts. \n",
    "\n",
    "\n",
    "\\n\\nACTION SELECTION PROMPT:\\n\n",
    "Select an action from the list of available actions that is most relevant to the given utterance. \n",
    "To decide the applicable action, use the following procedconsultantsure to systematically filter the most relevant action:\n",
    "1. Check the dialog_type. If it is an INSTRUCT, then narrow the list of actions to only include those actions that are ontic actions or world-modifying actions. If the utterance is a QUESTION, then narrow the list of actions to only include those actions that are querying actions. If the utterance is a STATEMENT, then narrow the list of actions to only include those actions that are epistemic actions, or actions that change the belief state of the listener. \n",
    "\n",
    "2. Then, compare the name and description of the action to the utterance. Narrow the list of actions to include only those with a semantically similar name or description to the utterance. \n",
    "\n",
    "3. If the narrowed list contains one action, then return its name. If it contains no actions, then return NONE. If it contains more than one action, then return AMBIGUOUS.\n",
    "\n",
    "\\n\\n PROPERTIES EXTRACTION\\n\n",
    "Identify descriptors in the utterance that refer to an entity which is an argument or parameter in the action.\n",
    "Use the following procedure:\n",
    "1. For each of the parameter types, identify what descriptive terms refer to the parameter type from the utterance. \n",
    "That is think of an entity is being referred to by the parameter types, and see what descriptors refer to this entity.\n",
    "2. Build a predicates for each descriptive terms. The predicates have a functor name that is the descriptor and a variable name.\n",
    "Hypothesize variable names (e.g., VAR0, VAR1, etc.). The predicates should be of the form `descriptive term(variable names)`\n",
    "3. Compose the predicates into a list \n",
    "4. Return the list as properties. \n",
    "\n",
    "Remember, these are descriptive properties (adjectives in a sense) and do not include cues for articles, determiners, pronouns etc.\n",
    "Also remember that functors cannot contain spaces, so replace spaces with underscores.\n",
    "\\n\\nEXAMPLE\\n\n",
    "utterance: Pick up the blue ball\n",
    "action: pickup\n",
    "parameter types: [\"object\"]\n",
    "properties: [\"blue(VAR0)\", \"ball(VAR0)\"]\n",
    "\\n\\n\n",
    "\n",
    "\\n\\n IDENTIFYING COGNITIVE STATUS OF ENTITIES IN REFERRING EXPRESSIONS\\n\n",
    "For each variable in the properties, decide which ONE of the following five cognitive statuses the variables in the below properties could fall into:\n",
    "statuses: [INFOCUS, ACTIVATED\", FAMILIAR, UINIQUELY_IDENTIFIABLE, REFERENTIAL, TYPE_IDENTIFIABLE]\n",
    "\n",
    "As shown in the table below, the Givenness Hierarchy is comprised of six hierarchically nested tiers of cognitive status, \n",
    "where information with one cognitive status can be inferred to also have all\n",
    "lower statuses. Each level of the GH is “cued” by a set\n",
    "of linguistic forms, as seen in the table. For example, the second\n",
    "row of the table shows that the definite use of “this” can be\n",
    "used to infer that the speaker assumes the referent to be at\n",
    "least activated to their interlocutor.\n",
    "\\n\\n\n",
    "Cognitive Status | Mnemonic Status | Form |\n",
    "-----------------|-----------------|------|\n",
    "INFOCUS | in the focus of attention | it |\n",
    "ACTIVATED | in short term memory | this,that,this N |\n",
    "FAMILIAR | in long term memory| that N |\n",
    "UNIQUELY IDENTIFIABLE | in long term memory  or new | the N |\n",
    "REFERENTIAL | new or hypothetical|  indefinite this N |\n",
    "TYPE IDENTIFIABLE | new or hypothetical | a N |\n",
    "\n",
    "Remember, we need an argument that argues in favor of the parse, but we need to talk about why parse may or may not work too. \n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "parse: \\n{parse}\\n\n",
    "argument:\n",
    "\"\"\"\n",
    "\n",
    "prompt_argument = PromptTemplate(\n",
    "    input_variables=[\"utterance\", \"parse\"],\n",
    "    template=template_argument\n",
    ")\n",
    "\n",
    "chain_argument = LLMChain(llm=llm, prompt=prompt_argument)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905fe292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for item in data:\n",
    "    parsed, output = parse(\"brad\", \"self\", item['utterance'], actions)\n",
    "    print(json.dumps(output, indent=2))   \n",
    "    print(\"PARSED: \", parsed,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70235e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain(utterance, parse):\n",
    "    # Generate explanation\n",
    "    print(\"Generating explanation\")\n",
    "    argument = chain_argument.run(utterance=utterance, parse=parse)\n",
    "    return argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd433d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot test\n",
    "\n",
    "utterance01 = \"pick up the ball\"\n",
    "parsed01 = parse(\"vasanth\", \"self\", utterance01, actions)\n",
    "explanation01 = explain(utterance01, parsed01)\n",
    "print(f\"Utterance: {utterance01}\\nParse: {parsed01[0]}\\n\\nExplanation: {explanation01}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86d16b",
   "metadata": {},
   "source": [
    "# Research problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911fc40b",
   "metadata": {},
   "source": [
    "1. Incomplete action imperatives (\"Stack the block\" ... on top of what?) \n",
    "2. No matching action\n",
    "3. No matching property\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790bfe6",
   "metadata": {},
   "source": [
    "# v0.1\n",
    "\n",
    "The goal of this is to get a basic version up and running\n",
    "\n",
    "Approach 1\n",
    "- Infer dialog act\n",
    "- Look only at action names at first (maybe descriptions), if ambiguous, then look at action signatures, if ambiguous look at pre/eff, if still ambiguous ask for clarification. \n",
    "    - Need to be more general here. The imperative could be specifying a sequence of actions (not sure DIARC can handle this anyway), or an action that cannot occur unless another action is performed first --> constant interaction with planner is needed. Sometimes action imperatives are even just goal directives in disguise (go to the door --> no goToDoor action, but at(door) is a possible state. How to convert \"go to\" to \"be at\". \"put two blue blocks on top of the red one\". \n",
    "\n",
    "\n",
    "idea\n",
    "- Check if action names exist, if not then reword the utterance as a goal, and see if it can be handled that way\n",
    "- If only one name exists, then check action signature and types\n",
    "    - if that works, then check if precons are met, and if so, perform action\n",
    "    - If not, then set precons as goal, plan and generate action sequence, and then also perform target action\n",
    "- If multiple matches, then check to see the \"best\" action signature\n",
    "    - Select best action signature by \n",
    "\n",
    "Other things:\n",
    "- Get a pddl-based planner involved --> external calls \n",
    "\n",
    "Variations\n",
    "- human involvement could include providing some set of conditions that specify several states, or they provide a a group of actions (a miniplan) and refer to that by a name\n",
    "- human could provide action performance constraints - do X, but don't touch Y.\n",
    "\n",
    "- Challenges with reference resolution .\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
