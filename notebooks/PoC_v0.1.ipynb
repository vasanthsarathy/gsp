{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4790bfe6",
   "metadata": {},
   "source": [
    "# v0.1\n",
    "\n",
    "The goal of this is to get a basic version up and running\n",
    "\n",
    "Approach 1\n",
    "- Infer dialog act\n",
    "- Look only at action names at first (maybe descriptions), if ambiguous, then look at action signatures, if ambiguous look at pre/eff, if still ambiguous ask for clarification. \n",
    "    - Need to be more general here. The imperative could be specifying a sequence of actions (not sure DIARC can handle this anyway), or an action that cannot occur unless another action is performed first --> constant interaction with planner is needed. Sometimes action imperatives are even just goal directives in disguise (go to the door --> no goToDoor action, but at(door) is a possible state. How to convert \"go to\" to \"be at\". \"put two blue blocks on top of the red one\". \n",
    "\n",
    "\n",
    "idea\n",
    "- Check if action names exist, if not then reword the utterance as a goal, and see if it can be handled that way\n",
    "- If only one name exists, then check action signature and types\n",
    "    - if that works, then check if precons are met, and if so, perform action\n",
    "    - If not, then set precons as goal, plan and generate action sequence, and then also perform target action\n",
    "- If multiple matches, then check to see the \"best\" action signature\n",
    "    - Select best action signature by \n",
    "\n",
    "Other things:\n",
    "- Get a pddl-based planner involved --> external calls \n",
    "\n",
    "Variations\n",
    "- human involvement could include providing some set of conditions that specify several states, or they provide a a group of actions (a miniplan) and refer to that by a name\n",
    "- human could provide action performance constraints - do X, but don't touch Y.\n",
    "\n",
    "- Challenges with reference resolution .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b31e584",
   "metadata": {},
   "source": [
    "Some examples\n",
    "\n",
    "INSTRUCT(commX,shafer,pass(shafer,commX,VAR0),{plate(VAR0),DEFINITE(VAR0)})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "273036bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot testing\n",
    "data = [{\"utterance\": \"Pick up the ball\", \"act\": \"INSTRUCT\"},\n",
    "        {\"utterance\": \"Get the ball\", \"act\": \"INSTRUCT\"},\n",
    "        {\"utterance\": \"Pick up a ball\", \"act\": \"INSTRUCT\"},\n",
    "        {\"utterance\": \"Pick up this blue ball\", \"act\": \"INSTRUCT\"},\n",
    "        {\"utterance\": \"Put the ball down on the table\", \"act\":\"INSTRUCT\"},\n",
    "       {\"utterance\": \"Get the circuit breaker on the work area\", \"act\": \"INSTRUCT\"},\n",
    "       {\"utterance\": \"First raise your arms\", \"act\": \"INSTRUCT\"},\n",
    "       {\"utterance\": \"Dempster, who do you trust?\", \"act\": \"QUESTION\"},\n",
    "       {\"utterance\": \"Dempster, do you see an object?\", \"act\": \"QUESTION\"},\n",
    "       {\"utterance\": \"The area behind you is safe\", \"act\": \"STATEMENT\"},\n",
    "       {\"utterance\": \"The object in front of you is a ball\", \"act\": \"STATEMENT\"},\n",
    "       {\"utterance\": \"It uses a medical caddy\", \"act\": \"STATEMENT\"}]\n",
    "\n",
    "intents = [\"INSTRUCT\", \"QUESTION\", \"STATEMENT\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c8ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\n",
    "    {'action': 'pickup',\n",
    "    'description': 'Picks up items',\n",
    "    'parameters': ['object'], \n",
    "    },\n",
    "    {'action': 'putdown',\n",
    "    'description': 'Puts down items',\n",
    "    'parameters': ['object'], \n",
    "    }\n",
    "]\n",
    "\n",
    "consultants = [\n",
    "    {'consultant': 'vision',\n",
    "    'properties': [\n",
    "        {'name': 'clear',\n",
    "         'parameters': ['object']\n",
    "        },\n",
    "        {'name': 'holding',\n",
    "         'parameters': ['object']\n",
    "        },\n",
    "        {'name': 'on',\n",
    "        'parameters': ['object', 'object']}\n",
    "    ]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b6c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of dictionaries, and a key, return the entry in the list that matches\n",
    "def find_dict_in_list(lst, key, target):\n",
    "    for item in lst:\n",
    "        if not key in item:\n",
    "            #print(\"Key not in Dict\")\n",
    "            return None\n",
    "        if item[key] == target:\n",
    "            return item\n",
    "    #print(\"Nothing found\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf4fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c54827b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0)\n",
    "llm = OpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "094f3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHAINS\n",
    "\n",
    "## (1) Dialog Act Classification \n",
    "\n",
    "template_dialog_act_classifier= \"\"\"\n",
    "Decide whether the utterance below from a speaker to a listener is an INSTRUCT, QUESTION or STATEMENT. \n",
    "An INSTRUCT is an imperative statement or a request by the speaker to have the listener do an action.\n",
    "A QUESTION is a query or request from a speaker for more information from the listener about the listeners knowledge, beliefs or perceptions\n",
    "A STATEMENT is a statement of fact or opinion that the speaker conveys to a listener. \n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "act:\n",
    "\"\"\"\n",
    "\n",
    "prompt_dialog_act_classifier = PromptTemplate(\n",
    "    input_variables=[\"utterance\"],\n",
    "    template=template_dialog_act_classifier\n",
    ")\n",
    "\n",
    "chain_dialog_act_classifier = LLMChain(llm=llm, prompt=prompt_dialog_act_classifier)\n",
    "\n",
    "\n",
    "## (2) Find relevant action from Action DB\n",
    "\n",
    "template_action_selector= \"\"\"\n",
    "Select an action from the list of available actions that is most relevant to the given utterance. \n",
    "To decide the applicable action, use the following procedure to systematically filter the most relevant action:\n",
    "1. Check the dialog_type. If it is an INSTRUCT, then narrow the list of actions to only include those actions that are ontic actions or world-modifying actions. If the utterance is a QUESTION, then narrow the list of actions to only include those actions that are querying actions. If the utterance is a STATEMENT, then narrow the list of actions to only include those actions that are epistemic actions, or actions that change the belief state of the listener. \n",
    "\n",
    "2. Then, compare the name and description of the action to the utterance. Narrow the list of actions to include only those with a semantically similar name or description to the utterance. \n",
    "\n",
    "3. If the narrowed list contains one action, then return its name. If it contains no actions, then return NONE. If it contains more than one action, then return AMBIGUOUS.\n",
    "\n",
    "\n",
    "\\n\\n LIST OF AVAILABLE ACTIONS \\n:\n",
    "{actions}\n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "dialog act: \\n{dialog_act}\\n\n",
    "action:\n",
    "\"\"\"\n",
    "\n",
    "prompt_action_selector = PromptTemplate(\n",
    "    input_variables=[\"utterance\", \"actions\", \"dialog_act\"],\n",
    "    template=template_action_selector\n",
    ")\n",
    "\n",
    "chain_action_selector = LLMChain(llm=llm, prompt=prompt_action_selector)\n",
    "\n",
    "\n",
    "## (3) Extract parameters\n",
    "###>>  Doing this programmatically from the actions list\n",
    "\n",
    "\n",
    "## (4) Identify a referrent object and its properties. \n",
    "### Given an utterance, an action, and its parameter types, return a set of properties \n",
    "\n",
    "template_referent_properties= \"\"\"\n",
    "Identify descriptors in the utterance that refer to an entity which is an argument or parameter in the action.\n",
    "Use the following procedure:\n",
    "1. For each of the parameter types, identify what descriptive terms refer to the parameter type from the utterance. \n",
    "That is think of an entity is being referred to by the parameter types, and see what descriptors refer to this entity.\n",
    "2. Build a predicates for each descriptive terms. The predicates have a functor name that is the descriptor and a variable name.\n",
    "Hypothesize variable names (e.g., VAR0, VAR1, etc.). The predicates should be of the form `descriptive term(variable names)`\n",
    "3. Compose the predicates into a list \n",
    "4. Return the list as properties. \n",
    "\n",
    "Remember, these are descriptive properties (adjectives in a sense) and do not include cues for articles, determiners, pronouns etc.\n",
    "Also remember that functors cannot contain spaces, so replace spaces with underscores.\n",
    "\\n\\nEXAMPLE\\n\n",
    "utterance: Pick up the blue ball\n",
    "action: pickup\n",
    "parameter types: [\"object\"]\n",
    "properties: [\"blue(VAR0)\", \"ball(VAR0)\"]\n",
    "\\n\\n\n",
    "utterance: \\n{utterance}\\n\n",
    "action: \\n{action}\\n\n",
    "parameter types: \\n{parameters}\\n\n",
    "properties:\n",
    "\"\"\"\n",
    "prompt_referent_properties = PromptTemplate(\n",
    "    input_variables=[\"utterance\", \"action\", \"parameters\"],\n",
    "    template=template_referent_properties\n",
    ")\n",
    "\n",
    "chain_referent_properties = LLMChain(llm=llm, prompt=prompt_referent_properties)\n",
    "\n",
    "\n",
    "\n",
    "## (5) Ref Res: For each variable determine it's cognitive status \n",
    "### For each variable we are figuring out the one or more givenness hierarchy statuses\n",
    "\n",
    "template_cognitive_status= \"\"\"\n",
    "For each variable in the properties, decide which ONE of the following five cognitive statuses the variables in the below properties could fall into:\n",
    "statuses: [INFOCUS, ACTIVATED\", FAMILIAR, UINIQUELY_IDENTIFIABLE, REFERENTIAL, TYPE_IDENTIFIABLE]\n",
    "\n",
    "As shown in the table below, the Givenness Hierarchy is comprised of six hierarchically nested tiers of cognitive status, \n",
    "where information with one cognitive status can be inferred to also have all\n",
    "lower statuses. Each level of the GH is “cued” by a set\n",
    "of linguistic forms, as seen in the table. For example, the second\n",
    "row of the table shows that the definite use of “this” can be\n",
    "used to infer that the speaker assumes the referent to be at\n",
    "least activated to their interlocutor.\n",
    "\\n\\n\n",
    "Cognitive Status | Mnemonic Status | Form |\n",
    "-----------------|-----------------|------|\n",
    "INFOCUS | in the focus of attention | it |\n",
    "ACTIVATED | in short term memory | this,that,this N |\n",
    "FAMILIAR | in long term memory| that N |\n",
    "UNIQUELY IDENTIFIABLE | in long term memory  or new | the N |\n",
    "REFERENTIAL | new or hypothetical|  indefinite this N |\n",
    "TYPE IDENTIFIABLE | new or hypothetical | a N |\n",
    "\\n\\n\n",
    "\n",
    "\n",
    "When deciding the one cognitive status for each variable, use the table above and compare the form (pronoun, determiner, article) of the utterance to its status.\n",
    "\\n\\nExample:\\n\n",
    "utterance: Pick up the blue ball\n",
    "properties: [\"blue(VAR0)\", \"ball(VAR0)\"]\n",
    "cognitive status: [\"UNIQUELY IDENTIFIABLE(VAR0)\"]\n",
    "\\n\\n\n",
    "utterance: \\n{utterance}\\n\n",
    "properties: \\n{properties}\\n\n",
    "cognitive status:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt_cognitive_status = PromptTemplate(\n",
    "    input_variables=[\"utterance\", \"properties\"],\n",
    "    template=template_cognitive_status\n",
    ")\n",
    "\n",
    "chain_cognitive_status = LLMChain(llm=llm, prompt=prompt_cognitive_status)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## (6) Check if consultants can handle the properties \n",
    "### Basically, pass the properties in and have it \"reword\" it to map onto known consultant properties.\n",
    "### So, for example, if the system finds \"in_the_office(VAR0)\", but the consultant can handle inOffice(VAR0) then we need to map the properties onto that\n",
    "\n",
    "\n",
    "\n",
    "## (6) Construct the final parse \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "909d0548",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Explanations\n",
    "### Given a set of instructions that produced a parse, generate an argument for the parse is correct. \n",
    "\n",
    "template_argument = \"\"\"\n",
    "Generate an argume\n",
    "Cognitive Status | Mnemonic Status | Form |\n",
    "-----------------|-----------------|------|\n",
    "INFOCUS | in the focus of attention | it |\n",
    "ACTIVATED | in short term memory | this,that,this N |\n",
    "FAMILIAR | in long term memory| that N |\n",
    "UNIQUELY IDENTIFIABLE | in long term memory  or new | the N |\n",
    "REFERENTIAL | nt for why the system came up with the particular parse of the utterance. \n",
    "To generate this argument ONLY use the below information, which contains prior prompts. \n",
    "\n",
    "\n",
    "\\n\\nACTION SELECTION PROMPT:\\n\n",
    "Select an action from the list of available actions that is most relevant to the given utterance. \n",
    "To decide the applicable action, use the following procedure to systematically filter the most relevant action:\n",
    "1. Check the dialog_type. If it is an INSTRUCT, then narrow the list of actions to only include those actions that are ontic actions or world-modifying actions. If the utterance is a QUESTION, then narrow the list of actions to only include those actions that are querying actions. If the utterance is a STATEMENT, then narrow the list of actions to only include those actions that are epistemic actions, or actions that change the belief state of the listener. \n",
    "\n",
    "2. Then, compare the name and description of the action to the utterance. Narrow the list of actions to include only those with a semantically similar name or description to the utterance. \n",
    "\n",
    "3. If the narrowed list contains one action, then return its name. If it contains no actions, then return NONE. If it contains more than one action, then return AMBIGUOUS.\n",
    "\n",
    "\\n\\n PROPERTIES EXTRACTION\\n\n",
    "Identify descriptors in the utterance that refer to an entity which is an argument or parameter in the action.\n",
    "Use the following procedure:\n",
    "1. For each of the parameter types, identify what descriptive terms refer to the parameter type from the utterance. \n",
    "That is think of an entity is being referred to by the parameter types, and see what descriptors refer to this entity.\n",
    "2. Build a predicates for each descriptive terms. The predicates have a functor name that is the descriptor and a variable name.\n",
    "Hypothesize variable names (e.g., VAR0, VAR1, etc.). The predicates should be of the form `descriptive term(variable names)`\n",
    "3. Compose the predicates into a list \n",
    "4. Return the list as properties. \n",
    "\n",
    "Remember, these are descriptive properties (adjectives in a sense) and do not include cues for articles, determiners, pronouns etc.\n",
    "Also remember that functors cannot contain spaces, so replace spaces with underscores.\n",
    "\\n\\nEXAMPLE\\n\n",
    "utterance: Pick up the blue ball\n",
    "action: pickup\n",
    "parameter types: [\"object\"]\n",
    "properties: [\"blue(VAR0)\", \"ball(VAR0)\"]\n",
    "\\n\\n\n",
    "\n",
    "\\n\\n IDENTIFYING COGNITIVE STATUS OF ENTITIES IN REFERRING EXPRESSIONS\\n\n",
    "For each variable in the properties, decide which ONE of the following five cognitive statuses the variables in the below properties could fall into:\n",
    "statuses: [INFOCUS, ACTIVATED\", FAMILIAR, UINIQUELY_IDENTIFIABLE, REFERENTIAL, TYPE_IDENTIFIABLE]\n",
    "\n",
    "As shown in the table below, the Givenness Hierarchy is comprised of six hierarchically nested tiers of cognitive status, \n",
    "where information with one cognitive status can be inferred to also have all\n",
    "lower statuses. Each level of the GH is “cued” by a set\n",
    "of linguistic forms, as seen in the table. For example, the second\n",
    "row of the table shows that the definite use of “this” can be\n",
    "used to infer that the speaker assumes the referent to be at\n",
    "least activated to their interlocutor.\n",
    "\\n\\n\n",
    "Cognitive Status | Mnemonic Status | Form |\n",
    "-----------------|-----------------|------|\n",
    "INFOCUS | in the focus of attention | it |\n",
    "ACTIVATED | in short term memory | this,that,this N |\n",
    "FAMILIAR | in long term memory| that N |\n",
    "UNIQUELY IDENTIFIABLE | in long term memory  or new | the N |\n",
    "REFERENTIAL | new or hypothetical|  indefinite this N |\n",
    "TYPE IDENTIFIABLE | new or hypothetical | a N |\n",
    "\n",
    "Remember, we need an argument that argues in favor of the parse, but we need to talk about why parse may or may not work too. \n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "parse: \\n{parse}\\n\n",
    "argument:\n",
    "\"\"\"\n",
    "\n",
    "prompt_argument = PromptTemplate(\n",
    "    input_variables=[\"utterance\", \"parse\"],\n",
    "    template=template_argument\n",
    ")\n",
    "\n",
    "chain_argument = LLMChain(llm=llm, prompt=prompt_argument)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8a36a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "def parse_utterance(utterance, actions):\n",
    "    #print(f\"Utterance: {utterance}\")\n",
    "    #print(f\"Actions: {actions}\")\n",
    "    dialog_act = chain_dialog_act_classifier.run(utterance=utterance)\n",
    "    action = chain_action_selector.run(utterance=utterance, actions=actions, dialog_act=dialog_act)\n",
    "    parameters = []\n",
    "    properties = []\n",
    "    cognitive_status = []\n",
    "    variables = []\n",
    "    if not action==\"NONE\" and not action==\"AMBIGUOUS\":\n",
    "        # Get params\n",
    "        relevant_action = find_dict_in_list(actions, 'action', action)\n",
    "        parameters = relevant_action['parameters']\n",
    "        \n",
    "        # Get properties\n",
    "        properties = chain_referent_properties.run(utterance=utterance, action=action, parameters=parameters)\n",
    "        properties = ast.literal_eval(properties)\n",
    "        \n",
    "        # Get list of variable names\n",
    "        def get_args(predicate):\n",
    "            return re.search('\\(([^)]+)', predicate).group(1)\n",
    "        \n",
    "        if properties:\n",
    "            variables = list(set([get_args(x) for x in properties]))\n",
    "        \n",
    "        \n",
    "        cognitive_status = chain_cognitive_status.run(utterance=utterance, properties=properties)\n",
    "        \n",
    "        #eval cog status\n",
    "        cognitive_status = ast.literal_eval(cognitive_status)\n",
    "        \n",
    "    output = {'utterance': utterance,\n",
    "              'dialog_act':dialog_act,\n",
    "              'action': action,\n",
    "             'parameters': parameters,\n",
    "             'properties': properties,\n",
    "              'variables': variables,\n",
    "             'cognitive_status': cognitive_status}\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def parse(speaker, listener, utterance, actions):\n",
    "    output = parse_utterance(utterance, actions)\n",
    "    variables = \",\".join(output['variables'])\n",
    "    properties = \",\".join(output['properties'])\n",
    "    cognitive_status = \",\".join(output['cognitive_status'])\n",
    "    action = output['action']\n",
    "    dialog_act = output['dialog_act']\n",
    "    \n",
    "    template = \"{dialog_act}({speaker},{listener},{action}({variables}),{{{properties},{cognitive_status}}})\"\n",
    "    parsed = template.format(dialog_act=dialog_act,\n",
    "                            speaker=speaker,\n",
    "                            listener=listener,\n",
    "                            action=action,\n",
    "                            variables=variables,\n",
    "                            properties=properties,\n",
    "                            cognitive_status=cognitive_status)\n",
    "        \n",
    "    return parsed, output\n",
    "\n",
    "def explain(utterance, parse):\n",
    "    # Generate explanation\n",
    "    print(\"Generating explanation\")\n",
    "    argument = chain_argument.run(utterance=utterance, parse=parse)\n",
    "    return argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcd433d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating explanation\n",
      "Utterance: pick up the ball\n",
      "Parse: INSTRUCT(vasanth,self,pickup(VAR0),{ball(VAR0),UNIQUELY IDENTIFIABLE(VAR0)})\n",
      "\n",
      "Explanation: The system generated the parse ('INSTRUCT(vasanth,self,pickup(VAR0),{ball(VAR0),UNIQUELY IDENTIFIABLE(VAR0)})', {'utterance': 'pick up the ball', 'dialog_act': 'INSTRUCT', 'action': 'pickup', 'parameters': ['object'], 'properties': ['ball(VAR0)'], 'variables': ['VAR0'], 'cognitive_status': ['UNIQUELY IDENTIFIABLE(VAR0)']}) for the utterance \"pick up the ball\" based on the following reasoning:\n",
      "\n",
      "1. The dialog_act is 'INSTRUCT', which means the system should focus on ontic actions or world-modifying actions. The action 'pickup' is a world-modifying action, as it involves changing the state of the world by picking up an object.\n",
      "\n",
      "2. The action 'pickup' has a semantically similar name and description to the utterance \"pick up the ball\". The verb \"pick up\" in the utterance corresponds to the action 'pickup', and the object \"ball\" is a parameter of the action.\n",
      "\n",
      "3. The properties extraction process identified the descriptive term \"ball\" in the utterance, which refers to the parameter type 'object'. The system generated the predicate \"ball(VAR0)\" to represent this descriptor.\n",
      "\n",
      "4. The cognitive status of the variable VAR0 in the properties is determined to be 'UNIQUELY IDENTIFIABLE'. This is because the utterance uses the definite article \"the\" before \"ball\", which indicates that the speaker assumes the referent to be uniquely identifiable in the context of the conversation.\n",
      "\n",
      "The parse may not be perfect, as it does not account for any additional descriptors or properties of the ball that might be present in the context. However, based on the given information, the system generated a reasonable parse that captures the main elements of the utterance and the intended action.\n"
     ]
    }
   ],
   "source": [
    "# Spot test\n",
    "\n",
    "utterance01 = \"pick up the ball\"\n",
    "parsed01 = parse(\"vasanth\", \"self\", utterance01, actions)\n",
    "explanation01 = explain(utterance01, parsed01)\n",
    "print(f\"Utterance: {utterance01}\\nParse: {parsed01[0]}\\n\\nExplanation: {explanation01}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94549a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def nlu(utterance):\n",
    "    parsed, output = parse(\"brad\", \"self\", utterance, actions)\n",
    "    return parsed, output\n",
    "\n",
    "demo = gr.Interface(fn=nlu, inputs=\"text\", outputs=[\"text\", \"text\"])\n",
    "\n",
    "demo.launch() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2dcd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905fe292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for item in data:\n",
    "    parsed, output = parse(\"brad\", \"self\", item['utterance'], actions)\n",
    "    print(json.dumps(output, indent=2))   \n",
    "    print(\"PARSED: \", parsed,\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
