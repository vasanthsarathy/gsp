{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44deaadf",
   "metadata": {},
   "source": [
    "# Version 3\n",
    "\n",
    "For \"INSTRUCT\" speech acts, `tethering` involves:\n",
    "- comparing the linguistically derived parse (i.e., ling_parse) with available action signatures, and generating an association chain between the cpc_name and one or more corresponding actions \n",
    "    - ranked list of name matches. Filter down this list with argument matching. \n",
    "    - Failure here means agent cannot perform action\n",
    "\n",
    "For \"STATEMENT\" speech acts, `tethering` involves:\n",
    "- comparing the linguistically derived parse (i.e., ling_parse) with available concepts.\n",
    "    - Failure here means agent can learn a new fact, but not understand its meaning or be able to recognize the concept in a different setting, without further attempts at tethering. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe373ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Method\n",
    "# Given a list of dictionaries, and a key, return the entry in the list that matches\n",
    "def find_dict_in_list(lst, key, target):\n",
    "    for item in lst:\n",
    "        if not key in item:\n",
    "            #print(\"Key not in Dict\")\n",
    "            return None\n",
    "        if item[key] == target:\n",
    "            return item\n",
    "    #print(\"Nothing found\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1d5fb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\r\n"
     ]
    }
   ],
   "source": [
    "!echo $OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42ed162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI,ChatAnthropic\n",
    "from langchain.chains import LLMChain\n",
    "#import anthropic\n",
    "\n",
    "#from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b543c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM \n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0)\n",
    "#llm = OpenAI(temperature=0.0)\n",
    "#llm = Anthropic(model=\"claude-instant-1.1-100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37086b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (1) Speech Act Classification \n",
    "\n",
    "template_speech_act= \"\"\"\n",
    "Decide whether the utterance below from a speaker to a listener is one of \"want\", \"wantBel\", \"itk\"\n",
    "A \"want\" is an imperative statement or a request by the speaker to have the listener do an action or stop doing an action.\n",
    "An \"itk\" is a 'wh' or 'yes/no' query (what, why, when, where, who) or request from a speaker for more information from the listener about the listeners knowledge, beliefs or perceptions\n",
    "A \"wantBel\" is a statement of fact or opinion that the speaker conveys to a listener and  expects to listener to come to believe. \n",
    "\n",
    "\n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "act:\n",
    "\"\"\"\n",
    "\n",
    "prompt_speech_act = PromptTemplate(\n",
    "    input_variables=[\"utterance\"],\n",
    "    template=template_speech_act\n",
    ")\n",
    "\n",
    "chain_speech_act = LLMChain(llm=llm, prompt=prompt_speech_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9145543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (2) Central Referents \n",
    "\n",
    "template_centralref = \"\"\"\n",
    "What is the central item (which could be a single thing or a collection of things) that is being referred to in the below sentence?\n",
    "\n",
    "Remember, the central referent is a thing or object, not an action or descriptor.It is meant to capture the central real world item being referenced in the utterance. \n",
    "\n",
    "\n",
    "sentence: \\n{utterance}\\n \n",
    "referent:\n",
    "\"\"\"\n",
    "\n",
    "prompt_centralref = PromptTemplate(\n",
    "    input_variables=[\"utterance\"],\n",
    "    template=template_centralref\n",
    ")\n",
    "\n",
    "chain_centralref = LLMChain(llm=llm, prompt=prompt_centralref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9204930",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (3) Supporting Referents\n",
    "\n",
    "template_suppref = \"\"\"\n",
    "What are some objects (which could be a single thing or a collection of things) that is being referred to in the below sentence not including the central referent? Return as a python list.\n",
    "If none, then return empty list []. Even if only one item, return as a list.  \n",
    "Remember, the supporting referents are things or objects, not actions or descriptors. They are meant to capture the real world items being referenced in the utterance. \n",
    "\n",
    "\n",
    "sentence: \\n{utterance}\\n \n",
    "central referent: \\n{centralref}\\n\n",
    "supporting referents (noun(s) from utterance):\n",
    "\"\"\"\n",
    "\n",
    "prompt_suppref = PromptTemplate(\n",
    "    input_variables=[\"utterance\", \"centralref\"],\n",
    "    template=template_suppref\n",
    ")\n",
    "\n",
    "chain_suppref = LLMChain(llm=llm, prompt=prompt_suppref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc907d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (4) Getting the type of thing that the referents are \n",
    "\n",
    "template_typeof = \"\"\"\n",
    "Determine whether or not the referent item mentioned below in the context of the provided utterance is one of the types also provided below. To check if the referent is of a type, follow the below procedure\n",
    "1. Iterate through each item mentioned in the list of types. \n",
    "2. For each item X in the list of types expand on the meaning of each item, and then ask if the central referent is of type X given that meaning. \n",
    "3. If the central referent is of type X in the list, return X.\n",
    "\n",
    "\\n\\n EXAMPLE \\n\n",
    "utterance: The lemon is on the table\n",
    "referent: lemon\n",
    "types: ['area', 'physobj', 'location', 'pose']\n",
    "typeOf: Looking through the items in the list of types above. physobj is a physical object. lemon is a type of physical object. So, it is of type physobj\n",
    "\n",
    "Remember, return specifically ONE of the items in the list, or if none apply then return NONE. \n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "referent: \\n{ref}\\n\n",
    "types: \\n{types}\\n\n",
    "typeOf:\n",
    "\"\"\"\n",
    "\n",
    "prompt_typeof = PromptTemplate(\n",
    "    input_variables=[\"ref\", \"types\", \"utterance\"],\n",
    "    template=template_typeof\n",
    ")\n",
    "\n",
    "chain_typeof = LLMChain(llm=llm, prompt=prompt_typeof)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc4ea08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (5) Extract CPC\n",
    "\n",
    "template_cpc = \"\"\"\n",
    "Determine the core propositional content (cpc) of the utterance below in the context of its central referent and speech act type\n",
    "To do so, use the following procedure\n",
    "\n",
    "1. Determine the type of cpc (\"action\", \"concept\") associated with the utterance.\n",
    "If the speech act is a \"want\" that means the utterance is an imperative and the cpc is an \"action\".\n",
    "If the speech act is a \"wantBel\" (note the capital B) that means the utterance is a statement assertion, and the cpc will be a \"concept\"\n",
    "If the speech act is an \"itk\" that means the utterance contains a question about some concept, so the cpc is a \"concept\"\n",
    "\n",
    "2. If the type of cpc is an \"action\", then the core propositional content (or cpc) is the action that is being performed on the central referent.\n",
    "If the type of cpc is a \"concept\", then the core propositional content (or cpc) is a concept that is being associated with the central referent.\n",
    "\n",
    "3. Convert the cpc into a single representative word that captures its meaning, without any reference to the referents.\n",
    "\n",
    "4. return the converted cpc and its type in the following format \"<CPC>:<TYPE>\" \n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "speech act: \\n{speechact}\\n\n",
    "central referent: \\n{centralref}\n",
    "core propositional content and:\n",
    "\"\"\"\n",
    "\n",
    "prompt_cpc = PromptTemplate(\n",
    "    input_variables=[\"centralref\", \"utterance\",\"speechact\"],\n",
    "    template=template_cpc\n",
    ")\n",
    "\n",
    "chain_cpc = LLMChain(llm=llm, prompt=prompt_cpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9323172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (6) Candidate Real Actions \n",
    "## \"Real\" == actions implemented in the robot system. \n",
    "\"\"\"\n",
    "Approach: look to see if there exists an action that captures this.\n",
    "\n",
    "Criteria\n",
    "(1) Semantic similarity of Name \n",
    "(2) The arguments in the robot action exist in the linguistic parse. If not then we are either in the wrong action or we are missing an action\n",
    "\"\"\"\n",
    "\n",
    "template_candidate_realactions =\"\"\"\n",
    "Select a list of 5 candidate actions from the list of available actions that is most relevant to the core action performed on the central referent as understood in the context of the utterance. \n",
    "\n",
    "To decide the list of applicable candidate actions, use the following procedure to systematically filter the list of available actions:\n",
    "1. Compare the name and description (if any) of each action in the available actions to the core action. Narrow the list of actions to include only those with a semantically similar name or description to the central action. \n",
    "2. Return the narrowed list of actions as a python list of string action names followed by a colon and then a numeric score between 0 and 1 signifying the semantic similarity between the name or description and the central action.\n",
    "For example \"move:0.5\" where \"move\" is the action name and 0.5 is the similarity score. \n",
    "\n",
    "\\n\\n LIST OF AVAILABLE ACTIONS \\n:\n",
    "{actions}\n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "central referent: \\n{centralref}\\n\n",
    "core action: \\n{cpc}\\n\n",
    "candidate actions:\n",
    "\"\"\"\n",
    "\n",
    "prompt_candidate_realactions = PromptTemplate(\n",
    "    input_variables=[\"centralref\", \"utterance\",\"cpc\", \"actions\"],\n",
    "    template=template_candidate_realactions\n",
    ")\n",
    "\n",
    "chain_candidate_realactions = LLMChain(llm=llm, prompt=prompt_candidate_realactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61086150",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (7) Tether Real Action\n",
    "# provided a list of realarguments, bind referents to them. \n",
    "\n",
    "template_bound_action = \"\"\"\n",
    "Try to bind the candidate action's arguments to the central and supplementary referents. Use the following procedure:\n",
    "1. Look at the candidate action's arguments in order written as \"VAR<NUM>:<TYPE>\". If the first argument is of TYPE \"agent\", then bind that to \"self\".\n",
    "2. For the second argument (if it exists), if the central referent is an object of  type TYPE in the argument, then bind the central referent to the TYPE. If not, bind to NONE. \n",
    "3. For  any subsequent arguments, attempt to bind the supplementary referents in the same way. \n",
    "4. Return output as a python dictionary, with following format (Do NOT include any special characters like newlines):\n",
    "\"name\": \"<NAME OF THE ACTION>\",\"bindings\": [{{\"<VARIABLE NAME (E.g.VAR0)>\": \"<REFERENT>\"}}, ...]\n",
    "\n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "central referent: \\n{centralref}\\n\n",
    "supplementary referents: \\n{supprefs}\\n\n",
    "candidate action: \\n{candidate_full_info}\\n\n",
    "bound action:\n",
    "\"\"\"\n",
    "\n",
    "prompt_bound_action = PromptTemplate(\n",
    "    input_variables=[\"centralref\", \"supprefs\", \"utterance\",\"candidate_full_info\"],\n",
    "    template=template_bound_action\n",
    ")\n",
    "\n",
    "chain_bound_action = LLMChain(llm=llm, prompt=prompt_bound_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64038d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (6b) Candidate Real Concepts\n",
    "## \"Real\" == concepts understandable to a robotic system (some consultant exists for it)\n",
    "\"\"\"\n",
    "Approach: look to see if there exists an action that captures this.\n",
    "\n",
    "Criteria\n",
    "(1) Semantic similarity of Name \n",
    "(2) The arguments in the concept exist in the linguistic parse. If not then we are either in the wrong action or we are missing an action\n",
    "\"\"\"\n",
    "\n",
    "template_candidate_realconcepts=\"\"\"\n",
    "Select a list of 5 candidate concept from the list of available concepts that is most relevant to the core concept associated with the central referent as understood in the context of the utterance. \n",
    "\n",
    "To decide the list of applicable candidate concepts, use the following procedure to systematically filter the list of available concepts:\n",
    "1. Compare the name and description (if any) of each concept in the available concepts to the core concepts. Narrow the list of concepts to include only those with a semantically similar name or description to ONLY the core concept. \n",
    "2. Return the narrowed list of concepts as a python list of string concept names followed by a colon and then a numeric score between 0 and 1 signifying the semantic similarity between the name or description of the available concepts and the core concept.\n",
    "\n",
    "\\n\\n LIST OF AVAILABLE CONCEPTS \\n:\n",
    "{concepts}\n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "central referent: \\n{centralref}\\n\n",
    "core concept: \\n{cpc}\\n\n",
    "candidate concepts:\n",
    "\"\"\"\n",
    "\n",
    "prompt_candidate_realconcepts = PromptTemplate(\n",
    "    input_variables=[\"centralref\", \"utterance\",\"cpc\", \"concepts\"],\n",
    "    template=template_candidate_realconcepts\n",
    ")\n",
    "\n",
    "chain_candidate_realconcepts = LLMChain(llm=llm, prompt=prompt_candidate_realconcepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85ef2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (7b) Tether Real Concept, if available\n",
    "# provided a list of realarguments, bind referents to them. \n",
    "\n",
    "template_bound_concept = \"\"\"\n",
    "Try to bind each candidate concept's arguments to the central and supplementary referents. Use the following procedure:\n",
    "For each candidate concept: \n",
    "1. Look at its arguments in order written as \"VAR<NUM>:<TYPE>\". If the first argument is of TYPE \"agent\", then bind that to \"self\".\n",
    "2. For the second argument (if it exists), if the central referent is an object of  type TYPE in the argument, then bind the central referent to the TYPE. If not, bind to NONE. \n",
    "3. For  any subsequent arguments, attempt to bind the supplementary referents in the same way. \n",
    "4. Return output as a python dictionary, with following format (Do NOT include any special characters like newlines):\n",
    "\"name\": \"<NAME OF THE CONCEPT>\",\"bindings\": [{{\"<VARIABLE NAME (E.g.VAR0)>\": \"<REFERENT>\"}}, ...]\n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "central referent: \\n{centralref}\\n\n",
    "supplementary referents: \\n{supprefs}\\n\n",
    "candidate concepts: \\n{candidate_full_info}\\n\n",
    "bound concept:\n",
    "\"\"\"\n",
    "\n",
    "prompt_bound_concept = PromptTemplate(\n",
    "    input_variables=[\"centralref\", \"supprefs\", \"utterance\",\"candidate_full_info\"],\n",
    "    template=template_bound_concept\n",
    ")\n",
    "\n",
    "chain_bound_concept = LLMChain(llm=llm, prompt=prompt_bound_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92293219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (8) Novel concept induction\n",
    "\n",
    "template_novel_concept = \"\"\"\n",
    "Generate a concept template for the core concept within the context of the utterance. Use the following procedure:\n",
    "\n",
    "1. Extract a concept name. The name can be from the core concept itself. \n",
    "2. Generate a list of arguments, where each argument states the type of argument that can be bound to the concept.\n",
    "Here, we want to make sure that each argument type makes sense for the concept, and also can be bound to the central referent and zero or more of the supplemental references.\n",
    "\n",
    "Return output as a python dictionary, with following format (Do NOT include any special characters like newlines):\n",
    "\"name\": \"<NAME OF THE CORE CONCEPT>\",\"roles\": [{{\"<VARIABLE NAME (E.g.VAR0)>\": \"<TYPE>\"}}, ...]\n",
    "\n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "core concept: \\n{cpc}\\n\n",
    "types: \\n{types}\\n\n",
    "central referent: \\n{centralref}\\n\n",
    "supplementary referents: \\n{supprefs}\\n\n",
    "novel concept: \n",
    "\"\"\"\n",
    "\n",
    "prompt_novel_concept = PromptTemplate(\n",
    "    input_variables=[\"centralref\", \"supprefs\", \"utterance\", \"cpc\", \"types\"],\n",
    "    template=template_novel_concept\n",
    ")\n",
    "\n",
    "chain_novel_concept = LLMChain(llm=llm, prompt=prompt_novel_concept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68ba6073",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (9) SPC Property Candidate identification \n",
    "## getting the properties of interest\n",
    "## For each of the referents, we want to find any individual descriptors, we also want to find and apply any given relations between referents\n",
    "\n",
    "template_properties = \"\"\"\n",
    "Determine the properties of the referents mentioned in the bindings. Use the following procedure for each of the referents:\n",
    "1. The names of each of the referents itself should be added as a property to the list.\n",
    "2. From the utterance, extract all the adjectival descriptors used to describe the properties of the referents, and add to list.\n",
    "3. Add to this list, any prepositional relations (mentioned in the utterance) between two or more of the referents, not already covered by the semantics of the core propositional content. \n",
    "4. Return this list as a list of python dictionaries with the following format:\n",
    "\"name\": <NAME OF PROPERTY/DESCRIPTOR/RELATION>, \"arguments\": <LIST OF VARIABLE NAMES> \n",
    "\n",
    "where the variable names correspond to the variable names associated with each of the referents. Remember, the variable names have to be correct.\n",
    "\n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "referents: \\n{referent_info}\\n\n",
    "core propositional content: \\n{bound_candidate}\\n\n",
    "properties, descriptors and relations:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_properties = PromptTemplate(\n",
    "    input_variables=[\"referent_info\", \"utterance\", \"bound_candidate\"],\n",
    "    template=template_properties\n",
    ")\n",
    "\n",
    "chain_properties = LLMChain(llm=llm, prompt=prompt_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f9a0ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (10) Candidate real properties: Find the properties (SPCs) in the consultant properties. THese are things the robot perception/cognition can understand\n",
    "## \"Real\" == concepts understandable to a robotic system (some consultant exists for it)\n",
    "\n",
    "template_candidate_realprops=\"\"\"\n",
    "Select a list of 5 candidate concept from the list of available concepts that is most semantically similar to the property associated with the referent, as understood in the context of the utterance. \n",
    "\n",
    "To decide the list of applicable candidate concepts, use the following procedure to systematically filter the list of available concepts:\n",
    "1. Compare the name and description (if any) of each concept in the available concepts to the properties. Narrow the list of concepts to include only those with a semantically similar name or description to ONLY the property. \n",
    "2. Return the narrowed list of concepts as a python list of string concept names followed by a colon and then a numeric score between 0 and 1 signifying the semantic similarity between the name or description of the available concepts and the property.\n",
    "\n",
    "\\n\\n LIST OF AVAILABLE CONCEPTS \\n:\n",
    "{concepts}\n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "property: \\n{prop}\\n\n",
    "candidate concepts:\n",
    "\"\"\"\n",
    "\n",
    "prompt_candidate_realprops = PromptTemplate(\n",
    "    input_variables=[\"utterance\",\"prop\", \"concepts\"],\n",
    "    template=template_candidate_realprops\n",
    ")\n",
    "\n",
    "chain_candidate_realprops = LLMChain(llm=llm, prompt=prompt_candidate_realprops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b812bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (11) Cognitive Status\n",
    "\n",
    "template_cognitive_status= \"\"\"\n",
    "Determine the cognitive status of each of the referents mentioned in the bindings. Use the following procedure for each of the referents:\n",
    "\n",
    "1. Decide which ONE (and only one) of the following five cognitive statuses the referents could fall into:\n",
    "statuses: [INFOCUS, ACTIVATED\", FAMILIAR, DEFINITE, INDEFINITE]\n",
    "\n",
    "As shown in the table below, the Givenness Hierarchy is comprised of six hierarchically nested tiers of cognitive status, \n",
    "where information with one cognitive status can be inferred to also have all\n",
    "lower statuses. Each level of the GH is “cued” by a set\n",
    "of linguistic forms, as seen in the table. For example, the second\n",
    "row of the table shows that the definite use of “this” can be\n",
    "used to infer that the speaker assumes the referent to be at\n",
    "least activated to their interlocutor.\n",
    "\\n\\n\n",
    "Cognitive Status | Mnemonic Status | Form |\n",
    "-----------------|-----------------|------|\n",
    "INFOCUS | in the focus of attention | it |\n",
    "ACTIVATED | in short term memory | this,that,this N |\n",
    "FAMILIAR | in long term memory| that N |\n",
    "DEFINITE | in long term memory  or new | the N |\n",
    "INDEFINITE | new or hypothetical | a N |\n",
    "\\n\\n\n",
    "\n",
    "When deciding the one cognitive status for each referent, use the table above and compare the form (pronoun, determiner, article) of the utterance to its status.\n",
    "\n",
    "Return this list as a list of python dictionaries with the following format:\n",
    "\"name\": <COGNITIVE STATUS>, \"arguments\": <LIST OF VARIABLE NAMES>\n",
    "\n",
    "where the variable names correspond to the variable names associated with each of the referents. Remember, the variable names have to be correct.\n",
    "\n",
    "\n",
    "utterance: \\n{utterance}\\n\n",
    "referents: \\n{referent_info}\\n\n",
    "cognitive statuses:\n",
    "\"\"\"\n",
    "\n",
    "prompt_cognitive_status = PromptTemplate(\n",
    "    input_variables=[\"referent_info\", \"utterance\"],\n",
    "    template=template_cognitive_status\n",
    ")\n",
    "\n",
    "chain_cognitive_status = LLMChain(llm=llm, prompt=prompt_cognitive_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82498f3",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae67785",
   "metadata": {},
   "source": [
    "## Describing the linguistic parse algorithm in words\n",
    "\n",
    "The input to the algorithm is an `utterance`, `list of available referent types`, `available action repertoire` and `available sensory or conceptual repertoire`. Then we perform the following steps:\n",
    "\n",
    "1. Classify speech-act: This is the communicative intent of the speaker.\n",
    "2. Extract Central referent: What is the central real world item that is being discussed in the utterance\n",
    "3. Classify the \"type\" of the central referent (we find the type of object (e.g., physobj, location etc.) \n",
    "4. Extract Supplemental referents: What are some other real world objects or entities being discussed \n",
    "5. Classify the \"types\" of each of these supplemental references. \n",
    "6. Extract the core propositional content (`CPC`) of the utterance: The key `action` or `concept` being discussed in reference to the central referent and possibly other supplemental references. This is the key reason the speaker is even communicating to the listener -- to have them do some action X or inform them about some fact or concept Y. The X and Y here are the core propositional content \n",
    "7. Classify CPC as one of the grounded actions or concepts available in the robot. \n",
    "    - For actions: \n",
    "        1. Generate: Select a set of candidate actions from the available actions that match the CPC. \n",
    "        2. Tether: Attempt to bind each candidate action in the list to the referents ensuring that types and names match.\n",
    "    - For concepts:\n",
    "        1. Generate: Select a set of candidate concepts from the available concepts that match the CPC. If none, then generate a new symbol. \n",
    "        2. Tether: For each candidate concept, bind them. If novel concept, then no need to bind. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4baca",
   "metadata": {},
   "source": [
    "## Speeding up\n",
    "\n",
    "How can we speed up the parsing? We might be able to restrict how well and how much it searches\n",
    "\n",
    "1. (best | all): best option only selects one candidate that has the best name match. all looks through all actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06ccaa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "def clean_candidates(candidates):\n",
    "    \"\"\"\n",
    "    Cleans the list of candidates to extract a list of names and a list of scores of the candidates\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    scores = []\n",
    "    for candidate in candidates:\n",
    "        name = candidate.split(\":\")[0]\n",
    "        score = float(candidate.split(\":\")[1])\n",
    "        names.append(name)\n",
    "        scores.append(score)\n",
    "        \n",
    "    return names, scores\n",
    "\n",
    "\n",
    "def prune_candidates(names, scores, threshold=0.75):\n",
    "    return [(x,y) for x,y in zip(names,scores) if y > threshold ]\n",
    "\n",
    "def best_candidate(names, scores, threshold=0.75):\n",
    "    \"\"\"\n",
    "    Selects best name and score above a threshold. \n",
    "    \"\"\"\n",
    "    pruned_names = []\n",
    "    pruned_scores = []\n",
    "    for n,s in zip(names,scores):\n",
    "        if s>threshold:\n",
    "            pruned_names.append(n)\n",
    "            pruned_scores.append(s)\n",
    "    \n",
    "    if pruned_names:\n",
    "        return pruned_names[pruned_scores.index(max(pruned_scores))]\n",
    "    return \"NONE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8670acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import string\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.75\n",
    "\n",
    "def find_and_bind(utterance, actions, concepts, types):\n",
    "    print(f\"\\nProcessing utterance: {utterance}\")\n",
    "    print(\"[ ] Classifying speech act\", end=\"\\r\")\n",
    "    speech_act = chain_speech_act.run(utterance=utterance).lower()\n",
    "    print(\"[X] Classifying speech act\")\n",
    "    \n",
    "    # 2. Central Referent Extraction\n",
    "    print(\"[ ] Extracting referents\", end=\"\\r\")\n",
    "    centralref = chain_centralref.run(utterance=utterance).lower()\n",
    "    \n",
    "    centralreftype = chain_typeof.run(ref=centralref, types=types, utterance=utterance ).split(\" \")[-1]\n",
    "    centralreftype = centralreftype.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 3. Supporting Referents Extraction\n",
    "    supprefs = chain_suppref.run(utterance=utterance, centralref=centralref).lower()\n",
    "    supprefs = ast.literal_eval(supprefs)\n",
    "    \n",
    "    supprefs_full = [] #with type info\n",
    "    if supprefs:\n",
    "        for suppref in supprefs:\n",
    "            ref_type = chain_typeof.run(ref=suppref, types=types, utterance=utterance ).split(\" \")[-1]\n",
    "            supprefs_full.append(f\"{suppref}:{ref_type}\")\n",
    "    print(\"[X] Extracting referents\")\n",
    "    print(f\"\\tCentral ref: {centralref}:{centralreftype}\")\n",
    "    print(f\"\\tSuppl ref: {supprefs_full}\")\n",
    "\n",
    "    print(\"[ ] Extracting CPC\", end=\"\\r\")\n",
    "    cpc = chain_cpc.run(utterance=utterance, speechact=speech_act, centralref=centralref)\n",
    "    print(\"[X] Extracting CPC\")\n",
    "    print(f\"\\tcpc: {cpc}\")\n",
    "    \n",
    "    # 4. Find and Bind Candidate real actions and concepts to the CPC.\n",
    "    candidates = []\n",
    "    #bound_candidates = []\n",
    "    new_concept = {}\n",
    "    if \":action\" in cpc:\n",
    "        \n",
    "        # Find Candidates in the robot's action repertoire\n",
    "        print(\"[ ] Extracting Candidate actions\", end=\"\\r\")\n",
    "        candidates = chain_candidate_realactions.run(utterance=utterance, \n",
    "                                                       centralref=centralref,\n",
    "                                                       cpc=cpc,\n",
    "                                                       actions=actions) \n",
    "        candidates = ast.literal_eval(candidates)\n",
    "        print(\"[X] Extracting Candidate actions\")\n",
    "        print(f\"\\tCandidates: {candidates}\")\n",
    "\n",
    "        \n",
    "        # Bind Candidates\n",
    "        print(\"[ ] Selecting best candidate\", end=\"\\r\")\n",
    "        \n",
    "        # extract scores\n",
    "        names, scores = clean_candidates(candidates)\n",
    "        \n",
    "        # Select best candidate\n",
    "        best = best_candidate(names=names, scores=scores, threshold=SIMILARITY_THRESHOLD)\n",
    "        \n",
    "        print(\"[X] Selecting best candidate\")\n",
    "        print(f\"\\tBest: {best}\")\n",
    "        \n",
    "        print(\"[ ] Binding best candidate\", end=\"\\r\")\n",
    "        if not \"NONE\" in best:\n",
    "            # Bind best candidate to a real action\n",
    "            best_full = find_dict_in_list(actions, \"name\", best) #full info on the action \n",
    "            bound_candidate = chain_bound_action.run(utterance=utterance,\n",
    "                                                  centralref=centralref,\n",
    "                                                  supprefs=supprefs,\n",
    "                                                  candidate_full_info=find_dict_in_list(actions, \n",
    "                                                                                         \"name\", \n",
    "                                                                                         best))\n",
    "            # check if bound_candidate contains a \"NONE\"\n",
    "            if \"NONE\" in bound_candidate:\n",
    "                print(f\"We have a problem. There is a unbound variable here:\\n{bound_candidate}\")\n",
    "\n",
    "            # eval string\n",
    "            bound_candidate = ast.literal_eval(bound_candidate)\n",
    "        \n",
    "        print(\"[X] Binding best candidate\")\n",
    "        print(f\"\\tBest full: {best_full}\")\n",
    "        print(f\"\\tBound candidate: {bound_candidate}\")\n",
    "\n",
    "        \n",
    "    else:\n",
    "        # Find candidates in the robot's perceptual and conceptual repertoire\n",
    "        \n",
    "        print(\"[ ] Extracting Candidate concepts\", end=\"\\r\")\n",
    "        candidates = chain_candidate_realconcepts.run(utterance=utterance, \n",
    "                                                       centralref=centralref,\n",
    "                                                       cpc=cpc,\n",
    "                                                       concepts=concepts) \n",
    "        candidates = ast.literal_eval(candidates)\n",
    "        \n",
    "        print(\"[X] Extracting Candidate concepts\")\n",
    "        print(f\"\\tCandidates: {candidates}\")\n",
    "        \n",
    "        # Binding the best candidate above a certain threshold\n",
    "        ## Approach: \n",
    "        ## 1. Prune candidates to only include those with thresholds higher than SIMILARITY_THRESHOLD\n",
    "        ## 2. Bind the one with the highest score. If NONE in params then bind next one.\n",
    "        ## 3. Stop \n",
    "        \n",
    "        print(\"[ ] Selecting best candidate\", end=\"\\r\")\n",
    "        # extract scores\n",
    "        names, scores = clean_candidates(candidates)\n",
    "        \n",
    "        # Select best candidate\n",
    "        best = best_candidate(names=names, scores=scores, threshold=SIMILARITY_THRESHOLD)\n",
    "        \n",
    "        print(\"[X] Selecting best candidate\")\n",
    "        print(f\"\\tBest: {best}\")\n",
    "        \n",
    "        if not \"NONE\" in best:\n",
    "            # Bind best candidate to a real action\n",
    "            print(\"[ ] Binding best candidate\", end=\"\\r\")\n",
    "            best_full = find_dict_in_list(actions, \"name\", best)\n",
    "            bound_candidate = chain_bound_concept.run(utterance=utterance,\n",
    "                                                  centralref=centralref,\n",
    "                                                  supprefs=supprefs,\n",
    "                                                  candidate_full_info=find_dict_in_list(concepts, \n",
    "                                                                                         \"name\", \n",
    "                                                                                         best))\n",
    "            # check if bound_candidate contains a \"NONE\"\n",
    "            if \"NONE\" in bound_candidate:\n",
    "                print(f\"We have a problem. There is a unbound variable here:\\n{bound_candidate}\")\n",
    "\n",
    "            # eval string\n",
    "            bound_candidate = ast.literal_eval(bound_candidate)\n",
    "            \n",
    "            \n",
    "            print(\"[X] Binding best candidate\")\n",
    "            print(f\"\\tBest full: {best_full}\")\n",
    "            print(f\"\\tBound candidate: {bound_candidate}\")\n",
    "            \n",
    "        else:\n",
    "            # novel concept being described \n",
    "            # need to hypothesize a name and arguments. \n",
    "            print(\"[ ] Instantiating novel concept\", end=\"\\r\")\n",
    "            new_concept = chain_novel_concept.run(utterance=utterance,\n",
    "                                                 types=types,\n",
    "                                                 centralref=centralref,\n",
    "                                                 supprefs=supprefs,\n",
    "                                                 cpc=cpc)\n",
    "            \n",
    "            new_concept = ast.literal_eval(new_concept)\n",
    "            print(\"[X] Instantiating novel concept\")\n",
    "            \n",
    "            best_full = new_concept\n",
    "            \n",
    "            print(\"[ ] Binding novel concept\", end=\"\\r\")\n",
    "            bound_candidate = chain_bound_concept.run(utterance=utterance,\n",
    "                                                  centralref=centralref,\n",
    "                                                  supprefs=supprefs,\n",
    "                                                  candidate_full_info=find_dict_in_list([new_concept], \n",
    "                                                                                         \"name\", \n",
    "                                                                                         new_concept['name']))\n",
    "            \n",
    "            print(\"[X] Binding novel concept\")\n",
    "            \n",
    "            # check if bound_candidate contains a \"NONE\"\n",
    "            if \"NONE\" in bound_candidate:\n",
    "                print(f\"We have a problem. There is a unbound variable here:\\n{bound_candidate}\")\n",
    "            \n",
    "            bound_candidate = ast.literal_eval(bound_candidate)\n",
    "            \n",
    "            \n",
    "        print(\"[X] Selecting and Binding best concept candidate\")\n",
    "        print(f\"\\tBest full: {best_full}\")\n",
    "        print(f\"\\tBound candidate: {bound_candidate}\")\n",
    "\n",
    "    \n",
    "    # ----- PROPERTY BINDING ---------------\n",
    "    # Variable asignment (i.e., for each central and supp ref, their names need to have a consultant)\n",
    "    # also the variables need to match. \n",
    "    \n",
    "    print(\"[ ] Extracting Properties\", end=\"\\r\")\n",
    "    # Remove \"self\" from the binding list \n",
    "    bindings = []\n",
    "    for item in bound_candidate['bindings']:\n",
    "        if not list(item.values())[0] == 'self':\n",
    "            bindings.append(item)\n",
    "    \n",
    "    \n",
    "    \n",
    "    spc = chain_properties.run(utterance=utterance,\n",
    "                                     referent_info=bindings,\n",
    "                              bound_candidate=bound_candidate)\n",
    "    \n",
    "    spc = ast.literal_eval(spc)\n",
    "    \n",
    "    print(\"[X] Extracting Properties\")\n",
    "    print(f\"\\tspc: {spc}\")\n",
    "    print(f\"\\tBindings: {bindings}\")\n",
    "    \n",
    "    \n",
    "    # Finding consultant properties\n",
    "    print(\"[ ] Finding Consultant properties similar to SPC\", end=\"\\r\")\n",
    "    props_all = []\n",
    "    for prop in spc:\n",
    "    \n",
    "        candidate_consultant_properties = chain_candidate_realprops.run(utterance=utterance,\n",
    "                                                                   concepts=concepts,\n",
    "                                                                   prop=prop)\n",
    "    \n",
    "        candidate_consultant_properties = ast.literal_eval(candidate_consultant_properties)\n",
    "        \n",
    "        print(f\"\\tCand. Cons. Props. for {prop['name']}: {candidate_consultant_properties}\")\n",
    "        \n",
    "        # extract scores\n",
    "        names, scores = clean_candidates(candidate_consultant_properties)\n",
    "        \n",
    "        # Select best candidate\n",
    "        best = best_candidate(names=names, scores=scores, threshold=SIMILARITY_THRESHOLD)\n",
    "        \n",
    "        print(f\"\\tBest: {best}\")\n",
    "        \n",
    "        \n",
    "        props_all.append({'spc':prop, 'best':best})\n",
    "    \n",
    "    \n",
    "        \n",
    "    print(\"[X] Finding Consultants properties similar to SPC\")\n",
    "\n",
    "    print(f\"\\tProps all: {props_all}\")\n",
    "    \n",
    "    # Binding or Tethering variables in the best candidate property for each SPC\n",
    "    print(\"[ ] Binding Consultant properties similar to SPC\", end=\"\\r\")\n",
    "    \n",
    "    bound_properties = []\n",
    "    for prop in props_all: \n",
    "        if not \"none\" in prop['best'].lower():\n",
    "            bound_property = chain_bound_concept.run(utterance=utterance,\n",
    "                                                  centralref=centralref,\n",
    "                                                  supprefs=supprefs,\n",
    "                                                  candidate_full_info=find_dict_in_list(concepts, \n",
    "                                                                                         \"name\", \n",
    "                                                                                         prop['best']))\n",
    "            \n",
    "            # check if bound_candidate contains a \"NONE\"\n",
    "            if \"NONE\" in bound_property:\n",
    "                print(f\"We have a problem. There is a unbound variable here:\\n{bound_property}\")\n",
    "\n",
    "            # eval string\n",
    "            bound_property = ast.literal_eval(bound_property)\n",
    "            bound_properties.append(bound_property)\n",
    "    print(\"[X] Binding Consultant properties similar to SPC\")\n",
    "    print(f\"\\tBound Properties: {bound_properties}\")\n",
    "            \n",
    "    \n",
    "    # --------------------------------------\n",
    "        \n",
    "    \n",
    "    # ----------- COGNITIVE STATUS ------------------\n",
    "    # for each of central and supp referents.\n",
    "    \n",
    "    print(\"[ ] Classifying cognitive status\", end=\"\\r\")\n",
    "    cognitive_statuses = chain_cognitive_status.run(utterance=utterance,\n",
    "                                                   referent_info=bindings)\n",
    "    \n",
    "    cognitive_statuses = ast.literal_eval(cognitive_statuses)\n",
    "    print(\"[X] Classifying cognitive status\")\n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    output = {\n",
    "        \"utterance\": utterance,\n",
    "        \"speech_act\": speech_act,\n",
    "        \"centralref\": f\"{centralref}:{centralreftype}\",\n",
    "        \"supprefs\": supprefs_full,\n",
    "        \"cpc\": cpc,\n",
    "        \"candidates\": candidates,\n",
    "        \"bound_candidate\": bound_candidate,\n",
    "        \"best\": best_full,\n",
    "        \"spc\": spc,\n",
    "        \"cognitive_statuses\": cognitive_statuses,\n",
    "        'candidate_consultant_properties': props_all,\n",
    "        \"bound_properties\": bound_properties\n",
    "    }\n",
    "    #print(json.dumps(output, indent=2))\n",
    "    \n",
    "    return output\n",
    "\n",
    "def construct_parse_string(bindings, grammar=None):\n",
    "    \"\"\"\n",
    "    Given a parse dict of bindings, construct a string (ideally, we provide a grammar too)\n",
    "    Grammar check not implemented\n",
    "    \"\"\"\n",
    "    speaker = \"brad\"\n",
    "    \n",
    "    # CPC\n",
    "    cpc_template = \"{cpc_name}({cpc_variables})\"\n",
    "    cpc = cpc_template.format(cpc_name=bindings['best']['name'],\n",
    "                             cpc_variables=\",\".join([ list(x.keys())[0] for x in  bindings['best']['roles'] ] ))\n",
    "    \n",
    "    print(cpc)\n",
    "    \n",
    "    # SPC\n",
    "    if bindings['spc']:\n",
    "        spcs = []\n",
    "        for s in bindings['spc']:\n",
    "            spc_template = \"{spc_name}({spc_variables})\"\n",
    "            spc = spc_template.format(spc_name=s['name'],\n",
    "                                     spc_variables=\",\".join(s['arguments']))\n",
    "            spcs.append(spc)\n",
    "    \n",
    "    spc_all = \",\".join(spcs)\n",
    "    \n",
    "    print(spc_all)\n",
    "    \n",
    "    final_template = \"{speech_act}({speaker},{cpc},{{{spc},{cognitive_status}}})\"\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b9ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_parse_string(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5bc416",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7104ab9",
   "metadata": {},
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0534dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing classes and functions\n",
    "import json\n",
    "\n",
    "def process_data_item(json_item):\n",
    "    actions = json_item['promptInfo']['actions']\n",
    "    concepts = json_item['promptInfo']['properties']\n",
    "    return actions, concepts\n",
    "\n",
    "class DIARCDataset:\n",
    "    def __init__(self, annotations_file, types=None):\n",
    "        with open(annotations_file, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.data = self.data['utterances']\n",
    "        self.types = types\n",
    "        if not self.types:\n",
    "            # default types\n",
    "            self.types = [\"physobj\", \"agent\", \"location\", \"pose\", \"action\", \"number\", \"direction\", \"name\", \"string\"]\n",
    "    \n",
    "    def print_stats(self):\n",
    "        num_items = len(self.data)\n",
    "        print(f\"Number of utterances: {num_items}\")\n",
    "    \n",
    "    def get_item(self, idx):\n",
    "        actions = self.data[idx]['promptInfo']['actions']\n",
    "        concepts = self.data[idx]['promptInfo']['properties']\n",
    "        utterance = self.data[idx]['utteranceText']\n",
    "        desired_semantics = self.data[idx]['desiredSemantics']\n",
    "        \n",
    "        item = {\n",
    "            \"utterance\": utterance,\n",
    "            \"desired_semantics\": desired_semantics,\n",
    "            \"actions\": actions,\n",
    "            \"concepts\": concepts,\n",
    "            \"types\": self.types\n",
    "        \n",
    "        }\n",
    "        return item\n",
    "    \n",
    "    def xy(self):\n",
    "        \"\"\"\n",
    "        returns all the utterances and desired semantics\n",
    "        \"\"\"\n",
    "        utterances = []\n",
    "        desired_semantics = []\n",
    "        for item in data:\n",
    "            utt = item['utterance']\n",
    "            des = item['desired_semantics']\n",
    "            utterances.append(utt)\n",
    "            desired_semantics.append(des)\n",
    "        return utterances, desired_semantics\n",
    "            \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de7d6bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available utterances: ['then assemble the caddy', 'that m3 screw belongs to Evan', 'screw the m3 into that hole on the conveyor']\n"
     ]
    }
   ],
   "source": [
    "# Construct Sample dev dataset \n",
    "import json \n",
    "\n",
    "with open(\"../data/actions_short.json\", \"r\") as f:\n",
    "    actions_dev = json.load(f)\n",
    "with open(\"../data/properties.json\", \"r\") as f:\n",
    "    concepts_dev = json.load(f)\n",
    "types = [\"physobj\", \"agent\", \"location\", \"pose\", \"action\", \"number\", \"direction\", \"name\", \"string\"]\n",
    "\n",
    "utterances = [\"then assemble the caddy\",\n",
    "             \"that m3 screw belongs to Evan\",\n",
    "             \"screw the m3 into that hole on the conveyor\"]\n",
    "\n",
    "dataset = []\n",
    "for utt in utterances:\n",
    "    item = {\"utterance\": utt,\n",
    "           \"actions\": actions_dev,\n",
    "           \"concepts\": concepts_dev,\n",
    "           \"types\": types}\n",
    "    dataset.append(item)\n",
    "\n",
    "print(f\"Available utterances: {utterances}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91710b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing utterance: that m3 screw belongs to Evan\n",
      "[X] Classifying speech act\n",
      "[X] Extracting referents\n",
      "\tCentral ref: m3 screw:physobj\n",
      "\tSuppl ref: ['evan:agent.']\n",
      "[X] Extracting CPC\n",
      "\tcpc: belonging:concept\n",
      "[X] Extracting Candidate concepts\n",
      "\tCandidates: ['hole:0.1', 'prop:0.2', 'bottle:0.1', 'conveyor:0.1', 'work area:0.2']\n",
      "[X] Selecting best candidate\n",
      "\tBest: NONE\n",
      "[X] Instantiating novel concept\n",
      "[X] Binding novel concept\n",
      "[X] Selecting and Binding best concept candidate\n",
      "\tBest full: {'name': 'belonging', 'roles': [{'VAR0': 'physobj'}, {'VAR1': 'agent'}]}\n",
      "\tBound candidate: {'name': 'belonging', 'bindings': [{'VAR0': 'm3 screw'}, {'VAR1': 'evan'}]}\n",
      "[X] Extracting Properties\n",
      "\tspc: [{'name': 'm3 screw', 'arguments': ['VAR0']}, {'name': 'Evan', 'arguments': ['VAR1']}]\n",
      "\tBindings: [{'VAR0': 'm3 screw'}, {'VAR1': 'evan'}]\n",
      "\tCand. Cons. Props. for m3 screw: ['hole: 0.2', 'm3: 0.8', 'deepM3: 0.6', 'prop: 0.3', 'bottle: 0.1']\n",
      "\tBest: m3\n",
      "\tCand. Cons. Props. for Evan: ['hole: 0.1', 'm3: 0.8', 'deepM3: 0.6', 'bottle: 0.1', 'screw feeder: 0.3']\n",
      "\tBest: m3\n",
      "[X] Finding Consultants properties similar to SPC\n",
      "\tProps all: [{'spc': {'name': 'm3 screw', 'arguments': ['VAR0']}, 'best': 'm3'}, {'spc': {'name': 'Evan', 'arguments': ['VAR1']}, 'best': 'm3'}]\n",
      "[X] Binding Consultant properties similar to SPC\n",
      "\tBound Properties: [{'name': 'm3', 'bindings': [{'X': 'm3 screw'}]}, {'name': 'm3', 'bindings': [{'X': 'm3 screw'}]}]\n",
      "[X] Classifying cognitive status\n",
      "{\n",
      "  \"utterance\": \"that m3 screw belongs to Evan\",\n",
      "  \"speech_act\": \"wantbel\",\n",
      "  \"centralref\": \"m3 screw:physobj\",\n",
      "  \"supprefs\": [\n",
      "    \"evan:agent.\"\n",
      "  ],\n",
      "  \"cpc\": \"belonging:concept\",\n",
      "  \"candidates\": [\n",
      "    \"hole:0.1\",\n",
      "    \"prop:0.2\",\n",
      "    \"bottle:0.1\",\n",
      "    \"conveyor:0.1\",\n",
      "    \"work area:0.2\"\n",
      "  ],\n",
      "  \"bound_candidate\": {\n",
      "    \"name\": \"belonging\",\n",
      "    \"bindings\": [\n",
      "      {\n",
      "        \"VAR0\": \"m3 screw\"\n",
      "      },\n",
      "      {\n",
      "        \"VAR1\": \"evan\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"best\": {\n",
      "    \"name\": \"belonging\",\n",
      "    \"roles\": [\n",
      "      {\n",
      "        \"VAR0\": \"physobj\"\n",
      "      },\n",
      "      {\n",
      "        \"VAR1\": \"agent\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"spc\": [\n",
      "    {\n",
      "      \"name\": \"m3 screw\",\n",
      "      \"arguments\": [\n",
      "        \"VAR0\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Evan\",\n",
      "      \"arguments\": [\n",
      "        \"VAR1\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"cognitive_statuses\": [\n",
      "    {\n",
      "      \"name\": \"FAMILIAR\",\n",
      "      \"arguments\": [\n",
      "        \"VAR0\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"DEFINITE\",\n",
      "      \"arguments\": [\n",
      "        \"VAR1\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"candidate_consultant_properties\": [\n",
      "    {\n",
      "      \"spc\": {\n",
      "        \"name\": \"m3 screw\",\n",
      "        \"arguments\": [\n",
      "          \"VAR0\"\n",
      "        ]\n",
      "      },\n",
      "      \"best\": \"m3\"\n",
      "    },\n",
      "    {\n",
      "      \"spc\": {\n",
      "        \"name\": \"Evan\",\n",
      "        \"arguments\": [\n",
      "          \"VAR1\"\n",
      "        ]\n",
      "      },\n",
      "      \"best\": \"m3\"\n",
      "    }\n",
      "  ],\n",
      "  \"bound_properties\": [\n",
      "    {\n",
      "      \"name\": \"m3\",\n",
      "      \"bindings\": [\n",
      "        {\n",
      "          \"X\": \"m3 screw\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"m3\",\n",
      "      \"bindings\": [\n",
      "        {\n",
      "          \"X\": \"m3 screw\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "out = find_and_bind(utterance=dataset[idx]['utterance'],\n",
    "                       actions=dataset[idx]['actions'],\n",
    "                       concepts=dataset[idx]['concepts'],\n",
    "                       types=types)\n",
    "print(json.dumps(out, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f35aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for text in dev:\n",
    "    print(text)\n",
    "    out = linguistic_parse(text)\n",
    "    print(json.dumps(out, indent=2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e8a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_dict_in_list(actions, \"name\", \"mountScrew\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6d8bf",
   "metadata": {},
   "source": [
    "# Next\n",
    "\n",
    "- [ ] The SPCs are a bit of a mess. Need to fix up that datastructure\n",
    "- [ ] Variable names from the concepts.txt has \"X\", \"Y\" etc. We need to make sure they are instead VAR0, VAR1 etc. \n",
    "    - Align variable names in spc predicates with the cpc. \n",
    "    - Maybe consider a separate datastructure to store \"referent\" information \n",
    "        - referents, properties, cognitive statuses\n",
    "        \n",
    "        ```\n",
    "        referents = [\n",
    "            {\"text\": \"m3 screw\", \n",
    "            \"type\": \"physobj\",\n",
    "            \"variable_name\": \"VAR0\",\n",
    "            \"cognitive status\": \"ACTIVATED\"},\n",
    "            \n",
    "            {\"text\": \"evan\", \n",
    "            \"type\": \"agent\",\n",
    "            \"variable_name\": \"VAR1\",\n",
    "            \"cognitive status\": \"FAMILIAR\"}\n",
    "            ]\n",
    "            \n",
    "        descriptors = [\n",
    "            {\"text\": \"m3 screw\", \n",
    "            \"name\": \"m3\",\n",
    "            \"arguments\": [\"VAR0\"] },\n",
    "            \n",
    "            {\"text\": \"evan\", \n",
    "            \"name\": \"NONE\",\n",
    "            \"arguments\": [] }\n",
    "            ]\n",
    "        \n",
    "        intention = {\"speech_act\": \"wantBel\",\n",
    "                    \"proposition\":\n",
    "                                {\"text\": belonging\",\n",
    "                                \"type\": \"concept\",\n",
    "                                \"arguments\": [\"VAR0\", \"VAR1\"]}\n",
    "            }\n",
    "            }   \n",
    "        ```\n",
    "     - Consider other datastructures as well for the cpc, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836069c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
